{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23e54e-91d3-4af2-9185-2e29bb9eb5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb63bba25ff44208aba6d0ea4bc165a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities from logits:\n",
      "A (A photo of a cylinder left of a cone): 0.00%\n",
      "B (A photo of a cone right of a cylinder): 0.00%\n",
      "C (A photo of a cylinder right of a cone): 98.20%\n",
      "D (A photo of a cone left of a cylinder): 1.80%\n",
      "E (A photo of a sphere left of a cylinder): 0.00%\n",
      "plots saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "# Update font sizes globally\n",
    "plt.rcParams.update({\n",
    "    'font.size': 15,\n",
    "    'axes.titlesize': 17,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 13,\n",
    "    'legend.title_fontsize': 14,\n",
    "})\n",
    "\n",
    "# --- configurable inputs ---\n",
    "\n",
    "##### IMAGE 1 #####\n",
    "#labels1 = [\n",
    "#    \"A photo of a cube right of a cone\",\n",
    "#    \"A photo of a cube left of a cone\",\n",
    "#    \"A photo of a cone right of a cube\",\n",
    "#    \"A photo of a cube left of a sphere\",\n",
    "#    \"A photo of a cylinder right of a cone\",]\n",
    "#correct_label1 = \"A photo of a cube right of a cone\"\n",
    "#image_path1 = \"/home/bboulbarss/large_dataset/relational/ood_val/cube_right_cone/cube right cone/CLEVR_rel_000025.png\"\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "## Image 1 in final_gradcam\n",
    "## Relational\n",
    "#labels = [\n",
    "#    \"A photo of a cylinder left of a cone\",\n",
    "#    \"A photo of a cylinder right of a cone\",\n",
    "#    \"A photo of a cone left of a cylinder\",\n",
    "#    \"A photo of a cube right a cylinder\",\n",
    "#    \"A photo of a sphere right of a cone\",]\n",
    "#correct_label = \"A photo of a cylinder left of a cone\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "#\n",
    "# Two object\n",
    "#labels = [\n",
    "#    \"A photo of a purple cylinder\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a purple cone\",\n",
    "#    \"A photo of a red sphere\",\n",
    "#    \"A photo of a blue cube\"\n",
    "#]\n",
    "#correct_label = \"A photo of a purple cylinder\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 2 in final_gradcam\n",
    "# Relational\n",
    "#labels = [\n",
    "#    \"A photo of a cylinder left of a sphere\",\n",
    "#    \"A photo of a cylinder right of a sphere\",\n",
    "#    \"A photo of a sphere left of a cylinder\",\n",
    "#    \"A photo of a cube right a cone\",\n",
    "#    \"A photo of a sphere right of a cone\",]\n",
    "#correct_label = \"A photo of a cylinder left of a sphere\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "\n",
    "# Two object\n",
    "#labels = [\n",
    "#    \"A photo of a green sphere\",\n",
    "#    \"A photo of a blue sphere\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a red cone\",\n",
    "#    \"A photo of a purple cube\",\n",
    "#]\n",
    "#correct_label = \"A photo of a green sphere\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 3 in final_gradcam\n",
    "# Relational\n",
    "#labels = [\n",
    "#    \"A photo of a cone left of a cylinder\",\n",
    "#    \"A photo of a cylinder left of a cone\",\n",
    "#    \"A photo of a cone right of a cylinder\",\n",
    "#    \"A photo of a cone right of a sphere\",\n",
    "#    \"A photo of a cube left of a cylinder\"\n",
    "#]\n",
    "#correct_label = \"A photo of a cone left of a cylinder\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cylinder/cone left cylinder/CLEVR_rel_000634.png\"\n",
    "\n",
    "# Two object\n",
    "#labels = [\n",
    "#    \"A photo of a yellow cone\",\n",
    "#    \"A photo of a blue cone\",\n",
    "#    \"A photo of a yellow cylinder\",\n",
    "#    \"A photo of a red sphere\",\n",
    "#    \"A photo of a brown cube\",\n",
    "#]\n",
    "#correct_label = \"A photo of a yellow cone\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cylinder/cone left cylinder/CLEVR_rel_000634.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 4 in final_gradcam\n",
    "# Relational\n",
    "#labels = [\n",
    "#    \"A photo of a cone left of a cylinder\",\n",
    "#    \"A photo of a cone right of a cylinder\",\n",
    "#    \"A photo of a cylinder right of a cone\",\n",
    "#    \"A photo of a cube left of a sphere\",\n",
    "#    \"A photo of a cone left of a sphere\",\n",
    "#]\n",
    "#correct_label = \"A photo of a cone right of a cylinder\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/two_object/ood_test/yellow_cylinder_blue_cone/yellow cylinder/CLEVR_yellow_cylinder_blue_cone_015448.png\"\n",
    "\n",
    "# Two object\n",
    "#labels=[\n",
    "#    \"A photo of a yellow cylinder\",\n",
    "#    \"A photo of a yellow cube\",\n",
    "#    \"A photo of a blue cylinder\",\n",
    "#    \"A photo of a yellow cone\",\n",
    "#    \"A photo of a purple cylinder\"\n",
    "#]\n",
    "#correct_label = \"A photo of a yellow cylinder\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/two_object/ood_test/yellow_cylinder_blue_cone/yellow cylinder/CLEVR_yellow_cylinder_blue_cone_015448.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 5 in final_gradcam\n",
    "# Relational\n",
    "#labels = [\n",
    "#    \"A photo of a cone left of a cube\",\n",
    "#    \"A photo of a cone right of a cube\",\n",
    "#    \"A photo of a cube left of a cone\",\n",
    "#    \"A photo of a sphere right of a cylinder\",\n",
    "#    \"A photo of a cube left of a cylinder\",\n",
    "#]\n",
    "#correct_label = \"A photo of a cone left of a cube\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cube/cone left cube/CLEVR_rel_000466.png\"\n",
    "\n",
    "# Two object\n",
    "#labels=[\n",
    "#    \"A photo of a purple cube\",\n",
    "#    \"A photo of a cyan cube\",\n",
    "#    \"A photo of a purple cone\",\n",
    "#    \"A photo of a gray sphere\",\n",
    "#    \"A photo of a brown cylinder\"\n",
    "#]\n",
    "#correct_label = \"A photo of a purple cube\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cube/cone left cube/CLEVR_rel_000466.png\"\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# Image 5 in final_gradcam\n",
    "# Relational\n",
    "#labels = [\n",
    "#    \"A photo of a cylinder left of a cone\",\n",
    "#    \"A photo of a cone right of a cylinder\",\n",
    "#    \"A photo of a cylinder right of a cone\",\n",
    "#    \"A photo of a cone left of a cylinder\",\n",
    "#    \"A photo of a sphere left of a cylinder\"\n",
    "#]\n",
    "#correct_label = \"A photo of a cone left of a cylinder\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cylinder/cone left cylinder/CLEVR_rel_000498.png\"\n",
    "\n",
    "\n",
    "save_dir = \"/home/bboulbarss/gradcam_results/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# --- Load model and processor ---\n",
    "config = AutoConfig.from_pretrained(\"AIDC-AI/Ovis2-8B\", trust_remote_code=True)\n",
    "config.llm_attn_implementation = \"eager\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AIDC-AI/Ovis2-8B\",\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"\n",
    ").cuda()\n",
    "text_tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()\n",
    "\n",
    "# --- 1. Generate multimodal embeddings for PCA plot ---\n",
    "embeddings = []\n",
    "for label in labels:\n",
    "    query = f\"<image>\\n{label}\"\n",
    "    images = [Image.open(image_path)]\n",
    "    prompt, input_ids, pixel_values = model.preprocess_inputs(query, images, max_partition=9)\n",
    "    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).to(model.device)\n",
    "    input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(model.device)\n",
    "    pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)]  # Fix: Correct dtype and device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Use merged attention mask if available, else create one\n",
    "        if 'attention_mask' in outputs:\n",
    "            merged_attention_mask = outputs['attention_mask']\n",
    "        else:\n",
    "            merged_attention_mask = torch.ones(last_hidden_state.shape[:2], device=last_hidden_state.device)\n",
    "        \n",
    "        merged_attention_mask = merged_attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        \n",
    "        summed = torch.sum(last_hidden_state * merged_attention_mask, dim=1)\n",
    "        count = torch.clamp(merged_attention_mask.sum(dim=1), min=1e-9)\n",
    "        embedding = summed / count\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        embeddings.append(embedding.to(dtype=torch.float32).cpu().numpy())\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# --- Apply PCA ---\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# --- Plot PCA ---\n",
    "# Define colorblind palette for \"Correct\" and explicit red for \"Others\"\n",
    "colorblind_colors = sns.color_palette(\"colorblind\")\n",
    "palette = {\"original\": colorblind_colors[2], \"ft\": (1, 0, 0)}  # Explicit red for Others\n",
    "\n",
    "# -- Plot PCA result --\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Separate points into \"correct\" and \"others\"\n",
    "correct_indices = [i for i, label in enumerate(labels) if label == correct_label]\n",
    "other_indices = [i for i, label in enumerate(labels) if label != correct_label]\n",
    "\n",
    "# Plot \"correct\" points\n",
    "if correct_indices:\n",
    "    plt.scatter(\n",
    "        embeddings_2d[correct_indices, 0], embeddings_2d[correct_indices, 1],\n",
    "        c=[palette[\"original\"]],  # Color for correct label\n",
    "        s=200,  # Larger circle for correct label\n",
    "        marker='o',  # Circle marker\n",
    "        label=\"Correct Label\"  # Legend entry\n",
    "    )\n",
    "\n",
    "# Plot \"others\" points\n",
    "if other_indices:\n",
    "    plt.scatter(\n",
    "        embeddings_2d[other_indices, 0], embeddings_2d[other_indices, 1],\n",
    "        c=[palette[\"ft\"]],  # Pure red for other labels\n",
    "        s=100,  # Smaller circle for others\n",
    "        marker='o',  # Circle marker\n",
    "        label=\"Wrong Label\"  # Legend entry\n",
    "    )\n",
    "\n",
    "# Equalize axis scales to avoid distortion\n",
    "plt.axis('equal')\n",
    "\n",
    "# Compute offsets only once based on axis limits\n",
    "x_min, x_max = plt.xlim()\n",
    "y_min, y_max = plt.ylim()\n",
    "x_offset = 0.01 * (x_max - x_min)\n",
    "y_offset = 0.01 * (y_max - y_min)\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        (embeddings_2d[i, 0] + x_offset, embeddings_2d[i, 1] + y_offset),\n",
    "        fontsize=13,\n",
    "        alpha=0.8,\n",
    "        ha='left',\n",
    "        va='bottom',\n",
    "        wrap=True\n",
    "    )\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.title(\"PCA Visualization of Ovis Text Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"text_pca_ovis_plot.png\"), bbox_inches=\"tight\", dpi=500)\n",
    "plt.close()\n",
    "\n",
    "# --- 2. Generate output probabilities for bar chart ---\n",
    "# Shuffle labels to match normal inference\n",
    "labels = labels.copy()\n",
    "#np.random.shuffle(labels)\n",
    "num_labels = len(labels)\n",
    "\n",
    "# Construct MCQ prompt\n",
    "mcq_prompt = \"Task: Identify the correct label for this image from the following choices:\\n\" + \"\\n\".join(\n",
    "    [f\"{chr(65+i)}. {labels[i]}\" for i in range(num_labels)]\n",
    ") + \"\\nAnswer with the letter of the correct choice.\"\n",
    "full_query = f\"<image>\\n{mcq_prompt}\"\n",
    "\n",
    "#print(labels)\n",
    "#print(full_query)\n",
    "\n",
    "# Preprocess inputs\n",
    "images = [Image.open(image_path).convert(\"RGB\")]\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(full_query, images, max_partition=9)\n",
    "attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).to(model.device)\n",
    "input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)]\n",
    "\n",
    "# Debug input_ids\n",
    "#print(f\"Input IDs shape: {input_ids.shape}, device: {input_ids.device}\")\n",
    "#print(f\"Input IDs: {input_ids[0].cpu().numpy()}\")\n",
    "#print(f\"Attention mask shape: {attention_mask.shape}, device: {attention_mask.device}\")\n",
    "\n",
    "# Try partial decoding to avoid OverflowError\n",
    "try:\n",
    "    # Decode only the last few tokens to check the end of the prompt\n",
    "    last_tokens = input_ids[0, -10:].cpu().numpy()  # Last 10 tokens\n",
    "    decoded_last = [text_tokenizer.decode([int(token)], skip_special_tokens=False) for token in last_tokens]\n",
    "    #print(f\"Last 10 input tokens: {last_tokens}\")\n",
    "    #print(f\"Decoded last 10 tokens: {decoded_last}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error decoding input_ids: {e}\")\n",
    "\n",
    "# Define answer letters\n",
    "answer_letters = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "answer_token_ids = []\n",
    "for letter in answer_letters:\n",
    "    encoded = text_tokenizer.encode(letter, add_special_tokens=False)\n",
    "    if len(encoded) == 1:\n",
    "        answer_token_ids.append(encoded[0])\n",
    "    else:\n",
    "        print(f\"Warning: Letter '{letter}' encoded to multiple tokens: {encoded}\")\n",
    "        answer_token_ids.append(encoded[0])  # Fallback to first token\n",
    "#print(f\"Answer letters: {answer_letters}\")\n",
    "#print(f\"Answer token IDs: {answer_token_ids}\")\n",
    "#print(f\"Decoded answer tokens: {[text_tokenizer.decode([id], skip_special_tokens=True) for id in answer_token_ids]}\")\n",
    "\n",
    "# Forward pass to get logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        pixel_values=pixel_values,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=None\n",
    "    )\n",
    "    logits = outputs.logits[:, -1, :]  # [1, vocab_size], logits for next token\n",
    "    #print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Compute probabilities\n",
    "probs_all = torch.softmax(logits, dim=-1)[0]  # [vocab_size]\n",
    "probs_all = probs_all.to(dtype=torch.float32)  # Convert to float32 to avoid bfloat16 issues\n",
    "probs_for_answers = probs_all[answer_token_ids].to(dtype=torch.float32)  # Convert to float32\n",
    "probs = (probs_for_answers / probs_for_answers.sum()).cpu().numpy()  # Normalize\n",
    "\n",
    "# Print probabilities\n",
    "print(\"Probabilities from logits:\")\n",
    "for letter, prob, label in zip(answer_letters, probs, labels):\n",
    "    print(f\"{letter} ({label}): {prob*100:.2f}%\")\n",
    "\n",
    "# Debug top logits\n",
    "top_k = 5\n",
    "top_token_ids = torch.topk(logits[0], k=top_k).indices.cpu().numpy()\n",
    "top_probs = torch.topk(probs_all, k=top_k).values.cpu().numpy()  # Now works with float32\n",
    "#print(f\"Top {top_k} tokens by logit:\")\n",
    "#for tid, prob in zip(top_token_ids, top_probs):\n",
    "    #decoded = text_tokenizer.decode([tid], skip_special_tokens=True)\n",
    "    #print(f\"Token ID={tid}, Decoded='{decoded}', Probability={prob*100:.2f}%\")\n",
    "\n",
    "# Plotting\n",
    "sorted_indices = np.argsort(probs)[::-1]\n",
    "sorted_labels = [labels[i] for i in sorted_indices]\n",
    "sorted_probs = [probs[i] * 100 for i in sorted_indices]\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = ['green' if label == correct_label else 'red' for label in sorted_labels]\n",
    "plt.bar(sorted_labels, sorted_probs, color=colors)\n",
    "plt.title(f\"Ovis Label Probabilities for Image\")\n",
    "plt.xlabel(\"Answer Choices\")\n",
    "plt.ylabel(\"Probability (%)\", rotation=0, labelpad=40)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(np.linspace(0, 100, 6))\n",
    "legend_handles = [plt.Rectangle((0,0),1,1, color='green'), plt.Rectangle((0,0),1,1, color='red')] if correct_label in sorted_labels else [plt.Rectangle((0,0),1,1, color='red')]\n",
    "legend_labels = ['Correct Label', 'Other Labels'] if correct_label in sorted_labels else ['Other Labels']\n",
    "plt.legend(handles=legend_handles, labels=legend_labels, loc='upper right')\n",
    "plt.tight_layout()\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(save_dir, \"ovis_probabilities_bar_plot.png\"), bbox_inches=\"tight\", dpi=500)\n",
    "plt.close()\n",
    "print('plots saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61c55f-8b1a-4edf-9b47-9eb3bb6b8ec3",
   "metadata": {},
   "source": [
    "# PCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf83752-3993-4b38-83c0-0e7a41a2795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image PCA plot saved to: /home/bboulbarss/pca_plots/ovis/image_pca_ovis_plot_all.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Define base directory and set directories\n",
    "base_dir = \"/home/bboulbarss/large_dataset/relational\"\n",
    "sets = {\n",
    "    \"train\": os.path.join(base_dir, \"train\"),\n",
    "    \"val\": os.path.join(base_dir, \"ood_val\"),\n",
    "    \"test\": os.path.join(base_dir, \"ood_test\"),\n",
    "}\n",
    "\n",
    "# Define markers for each set\n",
    "markers = {\"train\": \"o\", \"val\": \"s\", \"test\": \"^\"}\n",
    "\n",
    "# Function to get image paths based on set and directory structure\n",
    "def get_image_paths(set_name, set_dir):\n",
    "    image_data = []\n",
    "    # Get classes, ignoring dot files\n",
    "    classes = [c for c in os.listdir(set_dir) if not c.startswith('.')]\n",
    "    if set_name == \"train\":\n",
    "        for cls in classes:\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get images, ignoring dot files\n",
    "            images = [img for img in os.listdir(cls_dir) if not img.startswith('.')]\n",
    "            for img in images:\n",
    "                path = os.path.join(cls_dir, img)\n",
    "                image_data.append((path, cls, set_name))\n",
    "    else:  # val or test\n",
    "        for cls in classes:\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get the intermediate directory (assume there's only one, ignoring dot files)\n",
    "            intermediate_dirs = [d for d in os.listdir(cls_dir) if not d.startswith('.') and os.path.isdir(os.path.join(cls_dir, d))]\n",
    "            if intermediate_dirs:  # Ensure there's at least one intermediate directory\n",
    "                intermediate_dir = os.path.join(cls_dir, intermediate_dirs[0])\n",
    "                # Get images, ignoring dot files\n",
    "                images = [img for img in os.listdir(intermediate_dir) if not img.startswith('.')]\n",
    "                for img in images:\n",
    "                    path = os.path.join(intermediate_dir, img)\n",
    "                    image_data.append((path, cls, set_name))\n",
    "    return image_data\n",
    "\n",
    "# Collect all image data\n",
    "all_image_data = []\n",
    "for set_name, set_dir in sets.items():\n",
    "    image_data = get_image_paths(set_name, set_dir)\n",
    "    all_image_data.extend(image_data)\n",
    "\n",
    "# Extract image paths, classes, and sets\n",
    "image_paths, classes, sets_list = zip(*all_image_data)\n",
    "\n",
    "# Initialize a list to store embeddings for each image\n",
    "image_embeddings = []\n",
    "\n",
    "# Process each image with a minimal text prompt\n",
    "for img_path in image_paths:\n",
    "    query = \"<image>\"  # Minimal text prompt to focus on visual content\n",
    "    images = [Image.open(img_path)]\n",
    "    prompt, input_ids, pixel_values = model.preprocess_inputs(query, images, max_partition=9)\n",
    "    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).to(model.device)\n",
    "    input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(model.device)\n",
    "    pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Use merged attention mask if available, else create one\n",
    "        if 'attention_mask' in outputs:\n",
    "            merged_attention_mask = outputs['attention_mask']\n",
    "        else:\n",
    "            merged_attention_mask = torch.ones(last_hidden_state.shape[:2], device=last_hidden_state.device)\n",
    "        \n",
    "        merged_attention_mask = merged_attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        \n",
    "        # Compute the mean embedding\n",
    "        summed = torch.sum(last_hidden_state * merged_attention_mask, dim=1)\n",
    "        count = torch.clamp(merged_attention_mask.sum(dim=1), min=1e-9)\n",
    "        embedding = summed / count\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        image_embeddings.append(embedding.to(dtype=torch.float32).cpu().numpy())\n",
    "\n",
    "# Stack the embeddings into a single array\n",
    "image_embeddings = np.vstack(image_embeddings)\n",
    "\n",
    "# Apply PCA to reduce embeddings to 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(image_embeddings)\n",
    "\n",
    "# Get unique classes and assign colors\n",
    "unique_classes = sorted(set(classes))\n",
    "# Combine tab20, tab20b, and tab20c for up to 60 distinct colors\n",
    "colors = (plt.cm.tab20(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20b(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20c(np.linspace(0, 1, 20))[:, :3].tolist())\n",
    "class_colors = {cls: colors[i % len(colors)] for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Map each class to its set\n",
    "class_to_set = {cls: set_name for _, cls, set_name in all_image_data}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cls in unique_classes:\n",
    "    indices = [i for i, c in enumerate(classes) if c == cls]\n",
    "    x = embeddings_2d[indices, 0]\n",
    "    y = embeddings_2d[indices, 1]\n",
    "    color = class_colors[cls]\n",
    "    set_name = class_to_set[cls]\n",
    "    marker = markers[set_name]\n",
    "    plt.scatter(x, y, color=color, marker=marker, s=50)\n",
    "\n",
    "# Add legend for sets\n",
    "for set_name, marker in markers.items():\n",
    "    plt.scatter([], [], color='gray', marker=marker, label=set_name)\n",
    "plt.legend(title=\"Sets\")\n",
    "plt.axis('equal')\n",
    "plt.title(\"PCA Visualization of Ovis Image Embeddings\\n(Colors represent classes)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "image_pca_plot_path = os.path.join(save_dir, \"image_pca_ovis_plot_all.png\")\n",
    "plt.savefig(image_pca_plot_path, bbox_inches=\"tight\", dpi=500)\n",
    "plt.close()\n",
    "print(f\"Image PCA plot saved to: {image_pca_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0751eeb-d4df-498a-bf18-e365ce3d1226",
   "metadata": {},
   "source": [
    "# PCA plot, classes merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9e02a9-d90c-4e9a-a7cf-a34186f118fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image PCA plot saved to: /home/bboulbarss/pca_plots/ovis/image_pca_ovis_plot.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Define base directory and set directories\n",
    "base_dir = \"/home/bboulbarss/large_dataset/relational\"\n",
    "sets = {\n",
    "    \"train\": os.path.join(base_dir, \"train\"),\n",
    "    \"val\": os.path.join(base_dir, \"ood_val\"),\n",
    "    \"test\": os.path.join(base_dir, \"ood_test\"),\n",
    "}\n",
    "\n",
    "# Define markers for each set\n",
    "markers = {\"train\": \"o\", \"val\": \"s\", \"test\": \"^\"}\n",
    "\n",
    "# Function to compute canonical class name\n",
    "def get_canonical_class(cls):\n",
    "    parts = cls.split('_')\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Invalid class name format: {cls}\")\n",
    "    shape1, relation, shape2 = parts\n",
    "    if relation not in ['left', 'right']:\n",
    "        raise ValueError(f\"Invalid relation in class name: {cls}\")\n",
    "    if shape1 < shape2:\n",
    "        return cls\n",
    "    else:\n",
    "        inverted_relation = 'right' if relation == 'left' else 'left'\n",
    "        return shape2 + '_' + inverted_relation + '_' + shape1\n",
    "\n",
    "# Function to get image paths based on set and directory structure\n",
    "def get_image_paths(set_name, set_dir):\n",
    "    image_data = []\n",
    "    # Get classes, ignoring dot files\n",
    "    classes = [c for c in os.listdir(set_dir) if not c.startswith('.')]\n",
    "    if set_name == \"train\":\n",
    "        for cls in classes:\n",
    "            canonical_cls = get_canonical_class(cls)\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get images, ignoring dot files\n",
    "            images = [img for img in os.listdir(cls_dir) if not img.startswith('.')]\n",
    "            for img in images:\n",
    "                path = os.path.join(cls_dir, img)\n",
    "                image_data.append((path, canonical_cls, set_name))\n",
    "    else:  # val or test\n",
    "        for cls in classes:\n",
    "            canonical_cls = get_canonical_class(cls)\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get the intermediate directory (assume there's only one, ignoring dot files)\n",
    "            intermediate_dirs = [d for d in os.listdir(cls_dir) if not d.startswith('.') and os.path.isdir(os.path.join(cls_dir, d))]\n",
    "            if intermediate_dirs:  # Ensure there's at least one intermediate directory\n",
    "                intermediate_dir = os.path.join(cls_dir, intermediate_dirs[0])\n",
    "                # Get images, ignoring dot files\n",
    "                images = [img for img in os.listdir(intermediate_dir) if not img.startswith('.')]\n",
    "                for img in images:\n",
    "                    path = os.path.join(intermediate_dir, img)\n",
    "                    image_data.append((path, canonical_cls, set_name))\n",
    "    return image_data\n",
    "\n",
    "# Collect all image data\n",
    "all_image_data = []\n",
    "for set_name, set_dir in sets.items():\n",
    "    image_data = get_image_paths(set_name, set_dir)\n",
    "    all_image_data.extend(image_data)\n",
    "\n",
    "# Extract image paths, canonical classes, and sets\n",
    "image_paths, classes, sets_list = zip(*all_image_data)\n",
    "# Initialize a list to store embeddings for each image\n",
    "image_embeddings = []\n",
    "\n",
    "# Process each image with a minimal text prompt\n",
    "for img_path in image_paths:\n",
    "    query = \"<image>\"  # Minimal text prompt to focus on visual content\n",
    "    images = [Image.open(img_path)]\n",
    "    prompt, input_ids, pixel_values = model.preprocess_inputs(query, images, max_partition=9)\n",
    "    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).to(model.device)\n",
    "    input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(model.device)\n",
    "    pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Use merged attention mask if available, else create one\n",
    "        if 'attention_mask' in outputs:\n",
    "            merged_attention_mask = outputs['attention_mask']\n",
    "        else:\n",
    "            merged_attention_mask = torch.ones(last_hidden_state.shape[:2], device=last_hidden_state.device)\n",
    "        \n",
    "        merged_attention_mask = merged_attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        \n",
    "        # Compute the mean embedding\n",
    "        summed = torch.sum(last_hidden_state * merged_attention_mask, dim=1)\n",
    "        count = torch.clamp(merged_attention_mask.sum(dim=1), min=1e-9)\n",
    "        embedding = summed / count\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        image_embeddings.append(embedding.to(dtype=torch.float32).cpu().numpy())\n",
    "\n",
    "# Stack the embeddings into a single array\n",
    "image_embeddings = np.vstack(image_embeddings)\n",
    "\n",
    "# Apply PCA to reduce embeddings to 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(image_embeddings)\n",
    "\n",
    "# Get unique canonical classes and assign colors\n",
    "unique_classes = sorted(set(classes))\n",
    "# Combine tab20, tab20b, and tab20c for up to 60 distinct colors\n",
    "colors = (plt.cm.tab20(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20b(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20c(np.linspace(0, 1, 20))[:, :3].tolist())\n",
    "class_colors = {cls: colors[i % len(colors)] for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Map each canonical class to its set\n",
    "class_to_set = {cls: set_name for _, cls, set_name in all_image_data}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cls in unique_classes:\n",
    "    indices = [i for i, c in enumerate(classes) if c == cls]\n",
    "    x = embeddings_2d[indices, 0]\n",
    "    y = embeddings_2d[indices, 1]\n",
    "    color = class_colors[cls]\n",
    "    set_name = class_to_set[cls]\n",
    "    marker = markers[set_name]\n",
    "    plt.scatter(x, y, color=color, marker=marker, s=50)\n",
    "\n",
    "# Add legend for sets\n",
    "for set_name, marker in markers.items():\n",
    "    plt.scatter([], [], color='gray', marker=marker, label=set_name)\n",
    "plt.legend(title=\"Sets\")\n",
    "plt.axis('equal')\n",
    "plt.title(\"PCA Visualization of Ovis Image Embeddings\\n(Colors represent canonical classes)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "image_pca_plot_path = os.path.join(save_dir, \"image_pca_ovis_plot.png\")\n",
    "plt.savefig(image_pca_plot_path, bbox_inches=\"tight\", dpi=500)\n",
    "plt.close()\n",
    "print(f\"Image PCA plot saved to: {image_pca_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6983a8-693d-4432-ad1b-61ef5b2b3089",
   "metadata": {},
   "source": [
    "# PCA plot, classes merged, legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566be6e7-5415-49f8-af31-1bd039b55bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image PCA plot saved to: /home/bboulbarss/pca_plots/ovis/image_pca_ovis_plot_all.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Define base directory and set directories\n",
    "base_dir = \"/home/bboulbarss/large_dataset/relational\"\n",
    "sets = {\n",
    "    \"train\": os.path.join(base_dir, \"train\"),\n",
    "    \"val\": os.path.join(base_dir, \"ood_val\"),\n",
    "    \"test\": os.path.join(base_dir, \"ood_test\"),\n",
    "}\n",
    "\n",
    "# Define markers for each set\n",
    "markers = {\"train\": \"o\", \"val\": \"s\", \"test\": \"^\"}\n",
    "\n",
    "# Function to compute canonical class name\n",
    "def get_canonical_class(cls):\n",
    "    parts = cls.split('_')\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Invalid class name format: {cls}\")\n",
    "    shape1, relation, shape2 = parts\n",
    "    if relation not in ['left', 'right']:\n",
    "        raise ValueError(f\"Invalid relation in class name: {cls}\")\n",
    "    if shape1 < shape2:\n",
    "        return cls\n",
    "    else:\n",
    "        inverted_relation = 'right' if relation == 'left' else 'left'\n",
    "        return shape2 + '_' + inverted_relation + '_' + shape1\n",
    "\n",
    "# Function to get image paths based on set and directory structure\n",
    "def get_image_paths(set_name, set_dir):\n",
    "    image_data = []\n",
    "    # Get classes, ignoring dot files\n",
    "    classes = [c for c in os.listdir(set_dir) if not c.startswith('.')]\n",
    "    if set_name == \"train\":\n",
    "        for cls in classes:\n",
    "            canonical_cls = get_canonical_class(cls)\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get images, ignoring dot files\n",
    "            images = [img for img in os.listdir(cls_dir) if not img.startswith('.')]\n",
    "            for img in images:\n",
    "                path = os.path.join(cls_dir, img)\n",
    "                image_data.append((path, canonical_cls, set_name))\n",
    "    else:  # val or test\n",
    "        for cls in classes:\n",
    "            canonical_cls = get_canonical_class(cls)\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get the intermediate directory (assume there's only one, ignoring dot files)\n",
    "            intermediate_dirs = [d for d in os.listdir(cls_dir) if not d.startswith('.') and os.path.isdir(os.path.join(cls_dir, d))]\n",
    "            if intermediate_dirs:  # Ensure there's at least one intermediate directory\n",
    "                intermediate_dir = os.path.join(cls_dir, intermediate_dirs[0])\n",
    "                # Get images, ignoring dot files\n",
    "                images = [img for img in os.listdir(intermediate_dir) if not img.startswith('.')]\n",
    "                for img in images:\n",
    "                    path = os.path.join(intermediate_dir, img)\n",
    "                    image_data.append((path, canonical_cls, set_name))\n",
    "    return image_data\n",
    "\n",
    "# Collect all image data\n",
    "all_image_data = []\n",
    "for set_name, set_dir in sets.items():\n",
    "    image_data = get_image_paths(set_name, set_dir)\n",
    "    all_image_data.extend(image_data)\n",
    "\n",
    "# Extract image paths, canonical classes, and sets\n",
    "image_paths, classes, sets_list = zip(*all_image_data)\n",
    "\n",
    "# Initialize a list to store embeddings for each image\n",
    "image_embeddings = []\n",
    "\n",
    "# Process each image with a minimal text prompt\n",
    "for img_path in image_paths:\n",
    "    query = \"<image>\"  # Minimal text prompt to focus on visual content\n",
    "    images = [Image.open(img_path)]\n",
    "    prompt, input_ids, pixel_values = model.preprocess_inputs(query, images, max_partition=9)\n",
    "    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).to(model.device)\n",
    "    input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(model.device)\n",
    "    pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Use merged attention mask if available, else create one\n",
    "        if 'attention_mask' in outputs:\n",
    "            merged_attention_mask = outputs['attention_mask']\n",
    "        else:\n",
    "            merged_attention_mask = torch.ones(last_hidden_state.shape[:2], device=last_hidden_state.device)\n",
    "        \n",
    "        merged_attention_mask = merged_attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        \n",
    "        # Compute the mean embedding\n",
    "        summed = torch.sum(last_hidden_state * merged_attention_mask, dim=1)\n",
    "        count = torch.clamp(merged_attention_mask.sum(dim=1), min=1e-9)\n",
    "        embedding = summed / count\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        image_embeddings.append(embedding.to(dtype=torch.float32).cpu().numpy())\n",
    "\n",
    "# Stack the embeddings into a single array\n",
    "image_embeddings = np.vstack(image_embeddings)\n",
    "\n",
    "# Apply PCA to reduce embeddings to 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(image_embeddings)\n",
    "\n",
    "# Get unique canonical classes and assign colors\n",
    "unique_classes = sorted(set(classes))\n",
    "# Combine tab20, tab20b, and tab20c for up to 60 distinct colors\n",
    "colors = (plt.cm.tab20(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20b(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20c(np.linspace(0, 1, 20))[:, :3].tolist())\n",
    "class_colors = {cls: colors[i % len(colors)] for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Map each canonical class to its set\n",
    "class_to_set = {cls: set_name for _, cls, set_name in all_image_data}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))  # Slightly larger figure to accommodate two legends\n",
    "for cls in unique_classes:\n",
    "    indices = [i for i, c in enumerate(classes) if c == cls]\n",
    "    x = embeddings_2d[indices, 0]\n",
    "    y = embeddings_2d[indices, 1]\n",
    "    color = class_colors[cls]\n",
    "    set_name = class_to_set[cls]\n",
    "    marker = markers[set_name]\n",
    "    plt.scatter(x, y, color=color, marker=marker, s=50)\n",
    "\n",
    "# Add legend for sets\n",
    "for set_name, marker in markers.items():\n",
    "    plt.scatter([], [], color='gray', marker=marker, label=set_name)\n",
    "set_legend = plt.legend(title=\"Sets\", loc='upper left', bbox_to_anchor=(1.02, 1.0))\n",
    "\n",
    "# Add legend for classes\n",
    "class_handles = [plt.scatter([], [], color=class_colors[cls], marker='o', label=cls) for cls in unique_classes]\n",
    "class_legend = plt.legend(handles=class_handles, title=\"Classes\", loc='upper left', bbox_to_anchor=(1.02, 0.7))\n",
    "\n",
    "# Add both legends to the plot\n",
    "plt.gca().add_artist(set_legend)\n",
    "plt.axis('equal')\n",
    "plt.title(\"PCA Visualization of Ovis Image Embeddings\\n(Colors: Classes, Markers: Sets)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "image_pca_plot_path = os.path.join(save_dir, \"image_pca_ovis_plot_all.png\")\n",
    "plt.savefig(image_pca_plot_path, bbox_inches=\"tight\", dpi=500)\n",
    "plt.close()\n",
    "print(f\"Image PCA plot saved to: {image_pca_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7321141e-030b-4640-92e7-2d85cdae8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD TEXT PCA BAR PLOT\n",
    "\n",
    "#num_labels = len(labels)\n",
    "#mcq_prompt = \"Task: Identify the correct label for this image from the following choices:\\n\" + \"\\n\".join(\n",
    "#    [f\"{chr(65+i)}. {label}\" for i in range(num_labels)]\n",
    "#) + \"\\nAnswer with the letter of the correct choice.\"\n",
    "#full_query = f\"<image>\\n{mcq_prompt}\"\n",
    "#images = [Image.open(image_path)]\n",
    "#prompt, input_ids, pixel_values = model.preprocess_inputs(full_query, images, max_partition=9)\n",
    "#attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).to(model.device)\n",
    "#input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "#attention_mask = attention_mask.unsqueeze(0).to(model.device)\n",
    "#if pixel_values is not None:\n",
    "#    pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)]  # Fix: Correct dtype and device\n",
    "#with torch.no_grad():\n",
    "#    outputs = model(\n",
    "#        input_ids=input_ids,\n",
    "#        pixel_values=pixel_values,\n",
    "#        attention_mask=attention_mask,\n",
    "#        labels=None\n",
    "#    )\n",
    "#    answer_logits = outputs.logits[:, -1, :]  # Logits for the next token\n",
    "#    answer_token_ids = [text_tokenizer.convert_tokens_to_ids(letter) for letter in answer_letters]\n",
    "#    logits_for_answers = answer_logits[0, answer_token_ids]\n",
    "#    probs = torch.softmax(logits_for_answers.to(dtype=torch.float32), dim=0).cpu().numpy()\n",
    "#    print(f\"Probabilities: {dict(zip(answer_letters, probs*100))}\")\n",
    "#\n",
    "## Get token IDs for answer letters dynamically\n",
    "#answer_letters = [chr(65 + i) for i in range(num_labels)]\n",
    "#answer_token_ids = [text_tokenizer.convert_tokens_to_ids(letter) for letter in answer_letters]\n",
    "#logits_for_answers = logits[0, answer_token_ids]\n",
    "#probs = torch.softmax(logits_for_answers.to(dtype=torch.float32), dim=0).cpu().numpy()\n",
    "\n",
    "## --- Plot bar chart ---\n",
    "## Sort labels and probs by descending probability\n",
    "#sorted_indices = np.argsort(probs)[::-1]\n",
    "#sorted_labels = [labels[i] for i in sorted_indices]\n",
    "#sorted_probs = [probs[i] * 100 for i in sorted_indices]  # convert to percentages\n",
    "#\n",
    "## Plotting\n",
    "#plt.figure(figsize=(10, 7))\n",
    "## Create color list: green for correct_label, red for others\n",
    "#colors = ['green' if label == correct_label else 'red' for label in sorted_labels]\n",
    "#bars = plt.bar(sorted_labels, sorted_probs, color=colors)\n",
    "#\n",
    "## Titles and labels\n",
    "#plt.title(\"Ovis Label Probabilities for Image\")\n",
    "#plt.xlabel(\"Answer Choices\")\n",
    "#plt.ylabel(\"Probability (%)\", rotation=0, labelpad=40)\n",
    "#\n",
    "## Tick formatting\n",
    "#plt.xticks(rotation=45, ha=\"right\")\n",
    "#plt.yticks(np.linspace(0, 100, 6))\n",
    "#\n",
    "## Add legend\n",
    "#legend_handles = [plt.Rectangle((0,0),1,1, color='green'), plt.Rectangle((0,0),1,1, color='red')] if correct_label in sorted_labels else [plt.Rectangle((0,0),1,1, color='red')]\n",
    "#legend_labels = ['Correct Label', 'Other Labels'] if correct_label in sorted_labels else ['Other Labels']\n",
    "#plt.legend(handles=legend_handles, labels=legend_labels, loc='upper right')\n",
    "#\n",
    "## Layout and save\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(os.path.join(save_dir, \"ovis_probabilities_bar_plot.png\"), bbox_inches=\"tight\", dpi=500)\n",
    "#plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
