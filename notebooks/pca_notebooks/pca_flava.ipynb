{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d43711-d609-4a1c-9a80-c2a31da7df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bboulbarss/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "`input_ids_masked` isn't passed which means MLM loss won't be calculated correctlySetting it to `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if you are doing inference on unmasked text...\n",
      "/home/bboulbarss/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:1044: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "`input_ids_masked` isn't passed which means MLM loss won't be calculated correctlySetting it to `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if you are doing inference on unmasked text...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA plot saved to: /home/bboulbarss/pca_plots/flava/text_pca_flava_plot.png\n",
      "Bar plot saved to: /home/bboulbarss/pca_plots/flava/flava_probabilities_bar_plot.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import FlavaProcessor, FlavaForPreTraining\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "# Update font sizes globally\n",
    "plt.rcParams.update({\n",
    "    'font.size': 15,\n",
    "    'axes.titlesize': 17,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 13,\n",
    "    'legend.title_fontsize': 14,\n",
    "})\n",
    "\n",
    "# -- Prediction function --\n",
    "def flava_predict_label(image_path, texts, model, processor, device):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Replicate the image to match the number of text inputs (required for FLAVA)\n",
    "    images = [image] * len(texts)\n",
    "\n",
    "    # Process the inputs\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        max_length=77,\n",
    "        return_codebook_pixels=True,\n",
    "        return_image_mask=True,\n",
    "        return_attention_mask=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Get model outputs without gradient tracking for efficiency\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "\n",
    "    # Extract the image-text similarity scores\n",
    "    logits_per_image = output.contrastive_logits_per_image\n",
    "    # Convert logits to probabilities\n",
    "    probs = logits_per_image.softmax(dim=1)[0].unsqueeze(0)\n",
    "\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# -- User-configurable labels --\n",
    "\n",
    "##### IMAGE 1 #####\n",
    "#labels = [\n",
    "#    \"A photo of a cube right of a cone\",\n",
    "#    \"A photo of a cube left of a cone\",\n",
    "#    \"A photo of a cone right of a cube\",\n",
    "#    \"A photo of a cube left of a sphere\",\n",
    "#    \"A photo of a cylinder right of a cone\",]\n",
    "#correct_label = \"A photo of a cube right of a cone\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/ood_val/cube_right_cone/cube right cone/CLEVR_rel_000025.png\"\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "## Image 1 in final_gradcam\n",
    "## Relational\n",
    "labels = [\n",
    "    \"A photo of a cylinder left of a cone\",\n",
    "    \"A photo of a cylinder right of a cone\",\n",
    "    \"A photo of a cone left of a cylinder\",\n",
    "    \"A photo of a cube right of a cylinder\",\n",
    "    \"A photo of a sphere right of a cone\",]\n",
    "correct_label = \"A photo of a cylinder left of a cone\"\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "#\n",
    "# Two object\n",
    "#labels = [\n",
    "#    \"A photo of a purple cylinder\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a purple cone\",\n",
    "#    \"A photo of a red sphere\",\n",
    "#    \"A photo of a blue cube\"\n",
    "#]\n",
    "#correct_label = \"A photo of a purple cylinder\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 2 in final_gradcam\n",
    "# Relational\n",
    "#labels = [\n",
    "#    \"A photo of a cylinder left of a sphere\",\n",
    "#    \"A photo of a cylinder right of a sphere\",\n",
    "#    \"A photo of a sphere left of a cylinder\",\n",
    "#    \"A photo of a cube right a cone\",\n",
    "#    \"A photo of a sphere right of a cone\",]\n",
    "#correct_label = \"A photo of a cylinder left of a sphere\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "#\n",
    "# Two object\n",
    "#labels = [\n",
    "#    \"A photo of a green sphere\",\n",
    "#    \"A photo of a blue sphere\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a red cone\",\n",
    "#    \"A photo of a purple cube\",\n",
    "#]\n",
    "#correct_label = \"A photo of a green sphere\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Directory to save PCA plot\n",
    "save_dir = '/home/bboulbarss/pca_plots/flava'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "plot_path = os.path.join(save_dir, 'text_pca_flava_plot.png')\n",
    "\n",
    "# -- Load FLAVA model and processor --\n",
    "model_name = 'facebook/flava-full'\n",
    "model = FlavaForPreTraining.from_pretrained(model_name)\n",
    "processor = FlavaProcessor.from_pretrained(model_name)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# -- Load and preprocess the image --\n",
    "try:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to load or process image at {image_path}: {str(e)}\")\n",
    "\n",
    "# -- Create a list of images (same image for each label) --\n",
    "images = [image] * len(labels)\n",
    "\n",
    "# -- Prepare text and image inputs for embeddings --\n",
    "inputs = processor(text=labels, images=images, return_tensors='pt', padding=True, max_length=77, return_codebook_pixels=True, return_image_mask=True, return_attention_mask=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# -- Compute embeddings --\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # Extract the [CLS] token from text embeddings as sentence representation\n",
    "    text_embeddings = outputs.text_embeddings[:, 0, :]\n",
    "\n",
    "# -- Normalize embeddings for cosine-based analysis --\n",
    "text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "embeddings = text_embeddings.cpu().numpy()\n",
    "\n",
    "# -- Compute predictions using flava_predict_label --\n",
    "probs = flava_predict_label(image_path, labels, model, processor, device)\n",
    "#print(\"Prediction Probabilities:\")\n",
    "#for label, prob in zip(labels, probs[0]):\n",
    "#    print(f\"{label}: {prob:.4f}\")\n",
    "\n",
    "# sim_matrix = cosine_similarity(embeddings)\n",
    "# dist_matrix = cosine_distances(embeddings)\n",
    "# print(\"\\nCosine Similarity Matrix:\")\n",
    "# print(np.round(sim_matrix, 4))\n",
    "# print(\"\\nCosine Distance Matrix:\")\n",
    "# print(np.round(dist_matrix, 4))\n",
    "\n",
    "# -- PCA projection to 2 dimensions --\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# -- Plot PCA result --\n",
    "\n",
    "# Define colorblind palette for \"Correct\" and explicit red for \"Others\"\n",
    "colorblind_colors = sns.color_palette(\"colorblind\")\n",
    "palette = {\"original\": colorblind_colors[2], \"ft\": (1, 0, 0)}  # Explicit red for Others\n",
    "\n",
    "# -- Plot PCA result --\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Separate points into \"correct\" and \"others\"\n",
    "correct_indices = [i for i, label in enumerate(labels) if label ==  correct_label]\n",
    "other_indices = [i for i, label in enumerate(labels) if label !=  correct_label]\n",
    "\n",
    "# Plot \"correct\" points\n",
    "if correct_indices:\n",
    "    plt.scatter(\n",
    "        embeddings_2d[correct_indices, 0], embeddings_2d[correct_indices, 1],\n",
    "        c=[palette[\"original\"]],  # Color for correct label\n",
    "        s=200,  # Larger circle for correct label\n",
    "        marker='o',  # Circle marker\n",
    "        label=\"Correct Label\"  # Legend entry\n",
    "    )\n",
    "\n",
    "# Plot \"others\" points\n",
    "if other_indices:\n",
    "    plt.scatter(\n",
    "        embeddings_2d[other_indices, 0], embeddings_2d[other_indices, 1],\n",
    "        c=[palette[\"ft\"]],  # Pure red for other labels\n",
    "        s=100,  # Smaller circle for others\n",
    "        marker='o',  # Circle marker\n",
    "        label=\"Wrong Label\"  # Legend entry\n",
    "    )\n",
    "\n",
    "# Equalize axis scales to avoid distortion\n",
    "plt.axis('equal')\n",
    "\n",
    "# Compute offsets only once based on axis limits\n",
    "x_min, x_max = plt.xlim()\n",
    "y_min, y_max = plt.ylim()\n",
    "x_offset = 0.01 * (x_max - x_min)\n",
    "y_offset = 0.01 * (y_max - y_min)\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        (embeddings_2d[i, 0] + x_offset, embeddings_2d[i, 1] + y_offset),\n",
    "        fontsize=13,\n",
    "        alpha=0.8,\n",
    "        ha='left',\n",
    "        va='bottom',\n",
    "        wrap=True\n",
    "    )\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"PCA Visualization of FLAVA Text Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# -- Save and close plot --\n",
    "plt.savefig(plot_path, bbox_inches='tight', dpi=500)\n",
    "plt.close()\n",
    "\n",
    "print(f\"PCA plot saved to: {plot_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# -- Create bar plot for probabilities --\n",
    "# Sort labels and probabilities in descending order\n",
    "probs = probs[0]  # assuming shape (1, N)\n",
    "sorted_indices = np.argsort(probs)[::-1]\n",
    "sorted_labels = [labels[i] for i in sorted_indices]\n",
    "sorted_probs = [probs[i] * 100 for i in sorted_indices]  # convert to percentages\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "# Create color list: green for correct_label, red for others\n",
    "colors = ['green' if label == correct_label else 'red' for label in sorted_labels]\n",
    "bars = plt.bar(sorted_labels, sorted_probs, color=colors)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.ylabel(\"Probability (%)\", rotation=0, labelpad=40)\n",
    "plt.title(\"FLAVA Label Probabilities for Image\")\n",
    "\n",
    "# Ticks\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(np.linspace(0, 100, 6))\n",
    "\n",
    "# Add legend\n",
    "legend_handles = [plt.Rectangle((0,0),1,1, color='green'), plt.Rectangle((0,0),1,1, color='red')] if correct_label in sorted_labels else [plt.Rectangle((0,0),1,1, color='red')]\n",
    "legend_labels = ['Correct Label', 'Wrong Labels'] if correct_label in sorted_labels else ['Other Labels']\n",
    "plt.legend(handles=legend_handles, labels=legend_labels, loc='upper right')\n",
    "\n",
    "# Layout and save\n",
    "plt.tight_layout()\n",
    "bar_plot_path = os.path.join(save_dir, 'flava_probabilities_bar_plot.png')\n",
    "plt.savefig(bar_plot_path, bbox_inches='tight', dpi=500)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Bar plot saved to: {bar_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ede802-07f2-4d6b-bb02-cd932c55ce88",
   "metadata": {},
   "source": [
    "# PCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d39fcc-341e-441c-9eab-fc21a0650441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bboulbarss/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "`input_ids_masked` isn't passed which means MLM loss won't be calculated correctlySetting it to `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if you are doing inference on unmasked text...\n",
      "/home/bboulbarss/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:1044: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image PCA plot saved to: /home/bboulbarss/pca_plots/flava/image_pca_flava_plot_all.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Define base directory and set directories\n",
    "base_dir = \"/home/bboulbarss/large_dataset/relational\"\n",
    "sets = {\n",
    "    \"train\": os.path.join(base_dir, \"train\"),\n",
    "    \"val\": os.path.join(base_dir, \"ood_val\"),\n",
    "    \"test\": os.path.join(base_dir, \"ood_test\"),\n",
    "}\n",
    "\n",
    "# Define markers for each set\n",
    "markers = {\"train\": \"o\", \"val\": \"s\", \"test\": \"^\"}\n",
    "\n",
    "# Function to get image paths based on set and directory structure\n",
    "def get_image_paths(set_name, set_dir):\n",
    "    image_data = []\n",
    "    # Get classes, ignoring dot files\n",
    "    classes = [c for c in os.listdir(set_dir) if not c.startswith('.')]\n",
    "    if set_name == \"train\":\n",
    "        for cls in classes:\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get images, ignoring dot files\n",
    "            images = [img for img in os.listdir(cls_dir) if not img.startswith('.')]\n",
    "            for img in images:\n",
    "                path = os.path.join(cls_dir, img)\n",
    "                image_data.append((path, cls, set_name))\n",
    "    else:  # val or test\n",
    "        for cls in classes:\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get the intermediate directory (assume there's only one, ignoring dot files)\n",
    "            intermediate_dirs = [d for d in os.listdir(cls_dir) if not d.startswith('.') and os.path.isdir(os.path.join(cls_dir, d))]\n",
    "            if intermediate_dirs:  # Ensure there's at least one intermediate directory\n",
    "                intermediate_dir = os.path.join(cls_dir, intermediate_dirs[0])\n",
    "                # Get images, ignoring dot files\n",
    "                images = [img for img in os.listdir(intermediate_dir) if not img.startswith('.')]\n",
    "                for img in images:\n",
    "                    path = os.path.join(intermediate_dir, img)\n",
    "                    image_data.append((path, cls, set_name))\n",
    "    return image_data\n",
    "\n",
    "# Collect all image data\n",
    "all_image_data = []\n",
    "for set_name, set_dir in sets.items():\n",
    "    image_data = get_image_paths(set_name, set_dir)\n",
    "    all_image_data.extend(image_data)\n",
    "\n",
    "# Extract image paths, classes, and sets\n",
    "image_paths, classes, sets_list = zip(*all_image_data)\n",
    "\n",
    "# Load images\n",
    "images = [Image.open(path).convert('RGB') for path in image_paths]\n",
    "\n",
    "# Set dummy texts\n",
    "texts = [\"\"] * len(images)\n",
    "\n",
    "# Process inputs\n",
    "inputs = processor(text=texts, images=images, return_tensors='pt', padding=True, max_length=77, return_codebook_pixels=True, return_image_mask=True, return_attention_mask=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract image embeddings (assuming [CLS] token is at position 0)\n",
    "image_embeddings = outputs.image_embeddings[:, 0, :]\n",
    "\n",
    "# Normalize embeddings\n",
    "image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "embeddings = image_embeddings.cpu().numpy()\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Get unique classes and assign colors\n",
    "unique_classes = sorted(set(classes))\n",
    "# Combine tab20, tab20b, and tab20c for up to 60 distinct colors\n",
    "colors = (plt.cm.tab20(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20b(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20c(np.linspace(0, 1, 20))[:, :3].tolist())\n",
    "class_colors = {cls: colors[i % len(colors)] for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Map each class to its set\n",
    "class_to_set = {cls: set_name for _, cls, set_name in all_image_data}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cls in unique_classes:\n",
    "    indices = [i for i, c in enumerate(classes) if c == cls]\n",
    "    x = embeddings_2d[indices, 0]\n",
    "    y = embeddings_2d[indices, 1]\n",
    "    color = class_colors[cls]\n",
    "    set_name = class_to_set[cls]\n",
    "    marker = markers[set_name]\n",
    "    plt.scatter(x, y, color=color, marker=marker, s=25)\n",
    "\n",
    "# Add legend for sets\n",
    "for set_name, marker in markers.items():\n",
    "    plt.scatter([], [], color='gray', marker=marker, label=set_name)\n",
    "plt.legend(title=\"Sets\")\n",
    "plt.title(\"PCA Visualization of FLAVA Image Embeddings\\n(Colors represent classes)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "image_pca_plot_path = os.path.join(save_dir, \"image_pca_flava_plot_all.png\")\n",
    "plt.savefig(image_pca_plot_path, bbox_inches=\"tight\", dpi=500)\n",
    "plt.close()\n",
    "print(f\"Image PCA plot saved to: {image_pca_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a440f3b-1a2a-4178-83ed-bdf54ab069bb",
   "metadata": {},
   "source": [
    "# PCA plot, classes merged, legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c582e2b6-b9e8-408a-8342-875a12dd56ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bboulbarss/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "`input_ids_masked` isn't passed which means MLM loss won't be calculated correctlySetting it to `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if you are doing inference on unmasked text...\n",
      "/home/bboulbarss/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:1044: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 13.04 GiB. GPU 0 has a total capacity of 93.11 GiB of which 5.54 GiB is free. Process 2980266 has 1.96 GiB memory in use. Process 2996152 has 1.84 GiB memory in use. Including non-PyTorch memory, this process has 83.74 GiB memory in use. Of the allocated memory 67.00 GiB is allocated by PyTorch, and 16.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Get model outputs\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 84\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Extract image embeddings (assuming [CLS] token is at position 0)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m image_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mimage_embeddings[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/flava/modeling_flava.py:1906\u001b[0m, in \u001b[0;36mFlavaForPreTraining.forward\u001b[0;34m(self, input_ids, input_ids_masked, pixel_values, codebook_pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_unmasked_multimodal_encoder, mlm_labels, mim_labels, itm_labels, output_attentions, output_hidden_states, return_dict, return_loss)\u001b[0m\n\u001b[1;32m   1901\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m codebook_pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1902\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1903\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`codebook_pixel_value` are required to generate `mim_labels` if loss is expected. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1904\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall `AutoProcessor` with `return_codebook_pixels` set to True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1905\u001b[0m             )\n\u001b[0;32m-> 1906\u001b[0m         mim_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_codebook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_codebook_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodebook_pixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1907\u001b[0m \u001b[38;5;66;03m# Unimodal MIM Loss\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;66;03m# If multimodal embeddings are present, we will calculate MMM loss\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmim_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m image_masked_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m multimodal_masked_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/flava/modeling_flava.py:1595\u001b[0m, in \u001b[0;36mFlavaImageCodebook.get_codebook_indices\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_codebook_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;124;03m        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\u001b[38;5;241m.\u001b[39mformat(_CHECKPOINT_FOR_CODEBOOK_DOC)\n\u001b[0;32m-> 1595\u001b[0m     z_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(z_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/flava/modeling_flava.py:1508\u001b[0m, in \u001b[0;36mFlavaImageCodebookLayerGroup.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m-> 1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/flava/modeling_flava.py:1489\u001b[0m, in \u001b[0;36mFlavaImageCodebookBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m-> 1489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_path(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_gain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 13.04 GiB. GPU 0 has a total capacity of 93.11 GiB of which 5.54 GiB is free. Process 2980266 has 1.96 GiB memory in use. Process 2996152 has 1.84 GiB memory in use. Including non-PyTorch memory, this process has 83.74 GiB memory in use. Of the allocated memory 67.00 GiB is allocated by PyTorch, and 16.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Define base directory and set directories\n",
    "base_dir = \"/home/bboulbarss/large_dataset/relational\"\n",
    "sets = {\n",
    "    \"train\": os.path.join(base_dir, \"train\"),\n",
    "    \"val\": os.path.join(base_dir, \"ood_val\"),\n",
    "    \"test\": os.path.join(base_dir, \"ood_test\"),\n",
    "}\n",
    "\n",
    "# Define markers for each set\n",
    "markers = {\"train\": \"o\", \"val\": \"s\", \"test\": \"^\"}\n",
    "\n",
    "# Function to compute canonical class name\n",
    "def get_canonical_class(cls):\n",
    "    parts = cls.split('_')\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Invalid class name format: {cls}\")\n",
    "    shape1, relation, shape2 = parts\n",
    "    if relation not in ['left', 'right']:\n",
    "        raise ValueError(f\"Invalid relation in class name: {cls}\")\n",
    "    if shape1 < shape2:\n",
    "        return cls\n",
    "    else:\n",
    "        inverted_relation = 'right' if relation == 'left' else 'left'\n",
    "        return shape2 + '_' + inverted_relation + '_' + shape1\n",
    "\n",
    "# Function to get image paths based on set and directory structure\n",
    "def get_image_paths(set_name, set_dir):\n",
    "    image_data = []\n",
    "    # Get classes, ignoring dot files\n",
    "    classes = [c for c in os.listdir(set_dir) if not c.startswith('.')]\n",
    "    if set_name == \"train\":\n",
    "        for cls in classes:\n",
    "            canonical_cls = get_canonical_class(cls)\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get images, ignoring dot files\n",
    "            images = [img for img in os.listdir(cls_dir) if not img.startswith('.')]\n",
    "            for img in images:\n",
    "                path = os.path.join(cls_dir, img)\n",
    "                image_data.append((path, canonical_cls, set_name))\n",
    "    else:  # val or test\n",
    "        for cls in classes:\n",
    "            canonical_cls = get_canonical_class(cls)\n",
    "            cls_dir = os.path.join(set_dir, cls)\n",
    "            # Get the intermediate directory (assume there's only one, ignoring dot files)\n",
    "            intermediate_dirs = [d for d in os.listdir(cls_dir) if not d.startswith('.') and os.path.isdir(os.path.join(cls_dir, d))]\n",
    "            if intermediate_dirs:  # Ensure there's at least one intermediate directory\n",
    "                intermediate_dir = os.path.join(cls_dir, intermediate_dirs[0])\n",
    "                # Get images, ignoring dot files\n",
    "                images = [img for img in os.listdir(intermediate_dir) if not img.startswith('.')]\n",
    "                for img in images:\n",
    "                    path = os.path.join(intermediate_dir, img)\n",
    "                    image_data.append((path, canonical_cls, set_name))\n",
    "    return image_data\n",
    "\n",
    "# Collect all image data\n",
    "all_image_data = []\n",
    "for set_name, set_dir in sets.items():\n",
    "    image_data = get_image_paths(set_name, set_dir)\n",
    "    all_image_data.extend(image_data)\n",
    "\n",
    "# Extract image paths, canonical classes, and sets\n",
    "image_paths, classes, sets_list = zip(*all_image_data)\n",
    "\n",
    "# Load images\n",
    "images = [Image.open(path).convert('RGB') for path in image_paths]\n",
    "\n",
    "# Set dummy texts\n",
    "texts = [\"\"] * len(images)\n",
    "\n",
    "# Process inputs\n",
    "inputs = processor(text=texts, images=images, return_tensors='pt', padding=True, max_length=77, return_codebook_pixels=True, return_image_mask=True, return_attention_mask=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract image embeddings (assuming [CLS] token is at position 0)\n",
    "image_embeddings = outputs.image_embeddings[:, 0, :]\n",
    "\n",
    "# Normalize embeddings\n",
    "image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "embeddings = image_embeddings.cpu().numpy()\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Get unique canonical classes and assign colors\n",
    "unique_classes = sorted(set(classes))\n",
    "# Combine tab20, tab20b, and tab20c for up to 60 distinct colors\n",
    "colors = (plt.cm.tab20(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20b(np.linspace(0, 1, 20))[:, :3].tolist() + \n",
    "          plt.cm.tab20c(np.linspace(0, 1, 20))[:, :3].tolist())\n",
    "class_colors = {cls: colors[i % len(colors)] for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Map each canonical class to its set\n",
    "class_to_set = {cls: set_name for _, cls, set_name in all_image_data}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))  # Slightly larger figure to accommodate two legends\n",
    "for cls in unique_classes:\n",
    "    indices = [i for i, c in enumerate(classes) if c == cls]\n",
    "    x = embeddings_2d[indices, 0]\n",
    "    y = embeddings_2d[indices, 1]\n",
    "    color = class_colors[cls]\n",
    "    set_name = class_to_set[cls]\n",
    "    marker = markers[set_name]\n",
    "    plt.scatter(x, y, color=color, marker=marker, s=25)\n",
    "\n",
    "# Add legend for sets\n",
    "for set_name, marker in markers.items():\n",
    "    plt.scatter([], [], color='gray', marker=marker, label=set_name)\n",
    "set_legend = plt.legend(title=\"Sets\", loc='upper left', bbox_to_anchor=(1.02, 1.0))\n",
    "\n",
    "# Add legend for classes\n",
    "class_handles = [plt.scatter([], [], color=class_colors[cls], marker='o', label=cls) for cls in unique_classes]\n",
    "class_legend = plt.legend(handles=class_handles, title=\"Classes\", loc='upper left', bbox_to_anchor=(1.02, 0.7))\n",
    "\n",
    "# Add both legends to the plot\n",
    "plt.gca().add_artist(set_legend)\n",
    "plt.axis('equal')\n",
    "plt.title(\"PCA Visualization of FLAVA Image Embeddings\\n(Colors: Classes, Markers: Sets)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "image_pca_plot_path = os.path.join(save_dir, \"image_pca_flava_plot_all.png\")\n",
    "plt.savefig(image_pca_plot_path, bbox_inches=\"tight\", dpi=500)\n",
    "plt.close()\n",
    "print(f\"Image PCA plot saved to: {image_pca_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40690b-7120-40ee-9e8e-be9bf8fca6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
