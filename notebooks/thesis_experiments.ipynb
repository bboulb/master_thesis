{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sk3qZVIl_GL9",
    "outputId": "edc93d20-f932-4fab-f3af-fcce9512265f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from zoneinfo import ZoneInfo\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZI_ta9aN28L"
   },
   "source": [
    "## Predict functions for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4THW3RF-N2mA"
   },
   "outputs": [],
   "source": [
    "def clip_predict_label(image_path, labels, model, processor):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Process the inputs\n",
    "    inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the image-text similarity scores\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    # Convert logits to probabilities\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "    # Get the predicted label\n",
    "    predicted_label_idx = probs.argmax().item()\n",
    "    predicted_label = labels[predicted_label_idx]\n",
    "\n",
    "    return predicted_label, probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-AkPESJ35qVi"
   },
   "outputs": [],
   "source": [
    "def flava_predict_label(image_path, texts, model, processor):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Replicate the image to match the number of text inputs (required for FLAVA)\n",
    "    images = [image] * len(texts)\n",
    "\n",
    "    # Process the inputs\n",
    "    inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, max_length=77, return_codebook_pixels=True, return_image_mask=True, return_attention_mask=True).to(device)\n",
    "\n",
    "    # Get model outputs without gradient tracking for efficiency\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "\n",
    "    # Extract the image-text similarity scores\n",
    "    logits_per_image = output.contrastive_logits_per_image\n",
    "    # Convert logits to probabilities\n",
    "    probs = logits_per_image.softmax(dim=1)[0].unsqueeze(0)\n",
    "\n",
    "    # Get the predicted label\n",
    "    predicted_label_idx = probs.argmax().item()\n",
    "    predicted_label = texts[predicted_label_idx]\n",
    "\n",
    "    return predicted_label, probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qpC1n2fLeExw"
   },
   "outputs": [],
   "source": [
    "def vilt_predict_label(image_path, texts, model, processor):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Replicate the image to match the number of text inputs (required for ViLT)\n",
    "    images = [image] * len(texts)\n",
    "\n",
    "    # Process the inputs\n",
    "    inputs = processor(images, texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Get model outputs without gradient tracking for efficiency\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "\n",
    "    # Extract the image-text similarity scores\n",
    "    logits_per_image = output.logits\n",
    "    # Convert logits to probabilities\n",
    "    probs = logits_per_image.T.softmax(dim=1)\n",
    "\n",
    "    # Get the predicted label\n",
    "    predicted_label_idx = probs.argmax().item()\n",
    "    predicted_label = texts[predicted_label_idx]\n",
    "\n",
    "    return predicted_label, probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G-uhV-fhhTsM"
   },
   "outputs": [],
   "source": [
    "def idefics_predict_label(image_path, texts, model, processor):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Shuffle the texts list to randomize answer position\n",
    "    shuffled_texts = texts.copy()  # Create a copy to avoid modifying original list\n",
    "    random.shuffle(shuffled_texts)\n",
    "    \n",
    "    # Dynamically generate question string based on shuffled texts\n",
    "    question_string = \"Task: Identify the correct label for this image from the following choices:\\n\" + \"\\n\".join(\n",
    "        [f\"{chr(65+i)}. {text}\" for i, text in enumerate(shuffled_texts)]\n",
    "    ) + \"\\nAnswer with the letter of the correct choice.\\nAssistant:\"\n",
    "\n",
    "    # Prepare the input prompt\n",
    "    inputs = processor([\"User:\", image, question_string], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=8)\n",
    "\n",
    "    # Decode and return result\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_text, '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ovis_predict_label(image_path, texts, model, processor):\n",
    "    # Load and prepare the image\n",
    "    text_tokenizer, visual_tokenizer = processor\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    images = [image]\n",
    "\n",
    "    # Shuffle the texts to randomize answer position\n",
    "    shuffled_texts = texts.copy()\n",
    "    random.shuffle(shuffled_texts)\n",
    "\n",
    "    # Generate the question string with choices\n",
    "    question_string = \"Task: Identify the correct label for this image from the following choices:\\n\" + \"\\n\".join(\n",
    "        [f\"{chr(65+i)}. {text}\" for i, text in enumerate(shuffled_texts)]\n",
    "    ) + \"\\nAnswer with the letter of the correct choice.\"\n",
    "    query = f'<image>\\n{question_string}'\n",
    "\n",
    "    # Preprocess inputs for the Ovis model\n",
    "    max_partition = 9\n",
    "    prompt, input_ids, pixel_values = model.preprocess_inputs(query, images, max_partition=max_partition)\n",
    "    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id)\n",
    "\n",
    "    # Prepare inputs for generation\n",
    "    input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "    if pixel_values is not None:\n",
    "        pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "        pixel_values = [pixel_values]\n",
    "\n",
    "    # Define generation parameters\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"do_sample\": False,\n",
    "        \"eos_token_id\": model.generation_config.eos_token_id,\n",
    "        \"pad_token_id\": text_tokenizer.pad_token_id,\n",
    "        \"use_cache\": True\n",
    "    }\n",
    "\n",
    "    # Generate the output\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            **gen_kwargs\n",
    "        )[0]\n",
    "\n",
    "    generated_ids = output_ids\n",
    "    generated_text = text_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Construct the full sequence by appending generated text to the original query\n",
    "    full_sequence = query + generated_text\n",
    "\n",
    "    # Return the result in a format consistent with the Idefics function\n",
    "    return full_sequence, '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwNNahDypmdV"
   },
   "source": [
    "## Functions to create distractor labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K_Ayv9viAkVl"
   },
   "outputs": [],
   "source": [
    "def create_distractors_single_object(true_label):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    colors = ['blue', 'brown', 'cyan', 'gray', 'green', 'purple', 'red', 'yellow']\n",
    "\n",
    "    all_labels = [f\"A photo of a {color} {shape}\" for shape in shapes for color in colors]\n",
    "    all_labels.remove(true_label)\n",
    "\n",
    "    random_labels = random.sample(all_labels, k=K)\n",
    "\n",
    "    return random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QpS9pGGNBGbP"
   },
   "outputs": [],
   "source": [
    "def create_distractors_two_object(true_labels):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    colors = ['blue', 'brown', 'cyan', 'gray', 'green', 'purple', 'red', 'yellow']\n",
    "\n",
    "    _, _, _, _, color1, shape1 = true_labels[0].split()\n",
    "    _, _, _, _, color2, shape2 = true_labels[1].split()\n",
    "\n",
    "    hard_distractors = [f\"A photo of a {color1} {shape2}\", f\"A photo of a {color2} {shape1}\"]\n",
    "\n",
    "    exclude = set(true_labels + tuple(hard_distractors))\n",
    "    all_labels = [f\"A photo of a {color} {shape}\" for shape in shapes for color in colors if f\"A photo of a {color} {shape}\" not in exclude]\n",
    "    random_labels = random.sample(all_labels, k=K-len(hard_distractors))\n",
    "\n",
    "    return hard_distractors + random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ea3vaaskCQs9"
   },
   "outputs": [],
   "source": [
    "def create_distractors_relational(true_label):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    relations = {'right': 'left', 'left': 'right'}\n",
    "\n",
    "    true_parts = true_label.split()\n",
    "    _, _, _, _, true_shape1, true_relation, _, _, true_shape2 = true_parts  # e.g., 'a', 'photo, 'of', 'a', 'sphere', 'right', 'of', 'a', 'cube'\n",
    "\n",
    "    # Define hard distractors\n",
    "    # 1. Shape-swapped: Swap true_shape1 and true_shape2\n",
    "    shape_swapped = f\"A photo of a {true_shape2} {true_relation} of a {true_shape1}\"\n",
    "    # 2. Relation-swapped: Use opposite relation\n",
    "    relation_swapped = f\"A photo of a {true_shape1} {relations[true_relation]} of a {true_shape2}\"\n",
    "\n",
    "    hard_distractors = [shape_swapped, relation_swapped]\n",
    "\n",
    "    # Generate all possible labels\n",
    "    all_labels = [f\"A photo of a {shape} {rel} of a {other_shape}\"\n",
    "                  for shape in shapes\n",
    "                  for rel in relations\n",
    "                  for other_shape in shapes if other_shape != shape]\n",
    "\n",
    "    # Define the inverse label (already true and must be excluded)\n",
    "    inverse_label = f\" A photo of a {true_shape2} {relations[true_relation]} of a {true_shape1}\"\n",
    "\n",
    "    # Filter out true label, inverse label, and ensure hard distractors are unique\n",
    "    exclude = {true_label, inverse_label}\n",
    "    filtered_labels = [label for label in all_labels if label not in exclude]\n",
    "\n",
    "    # Sample random distractors, excluding hard distractors if they’re already in filtered_labels\n",
    "    random_labels = random.sample([label for label in filtered_labels if label not in hard_distractors], k=4-len(hard_distractors))\n",
    "\n",
    "    return hard_distractors + random_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHbre9BSMK3F"
   },
   "source": [
    "## Function to count number of images to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x8_GUDalMKMP"
   },
   "outputs": [],
   "source": [
    "def count_files_in_subdirs(directory, dataset):\n",
    "    root_dir = Path(directory)\n",
    "\n",
    "    subdir_counts = {}\n",
    "    total_files = 0\n",
    "\n",
    "    for subdir in root_dir.iterdir():\n",
    "        if subdir.is_dir():\n",
    "            if dataset=='two_object':\n",
    "                file_count = 0\n",
    "                for sub_subdir in subdir.iterdir():\n",
    "                    if sub_subdir.is_dir():\n",
    "                        file_count += sum(1 for item in sub_subdir.iterdir() if item.is_file())\n",
    "                subdir_counts[subdir.name] = file_count\n",
    "                total_files += file_count\n",
    "            elif dataset=='relational':\n",
    "                file_count = 0\n",
    "                for sub_subdir in subdir.iterdir():\n",
    "                    if sub_subdir.is_dir():\n",
    "                        file_count += sum(1 for item in sub_subdir.iterdir() if item.is_file())\n",
    "                subdir_counts[subdir.name] = file_count\n",
    "                total_files += file_count\n",
    "                \n",
    "            else:\n",
    "                file_count = sum(1 for item in subdir.iterdir() if item.is_file())\n",
    "                subdir_counts[subdir.name] = file_count\n",
    "                total_files += file_count\n",
    "\n",
    "    return total_files, subdir_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRz1AsGQ1frB"
   },
   "source": [
    "## Main experiment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_path_data = '/content/drive/MyDrive/thesis_small_dataset'\n",
    "google_path_experiment = '/content/drive/MyDrive/thesis_experiment_data'\n",
    "\n",
    "snellius_path_data = '/home/bboulbarss/large_dataset'\n",
    "snellius_path_experiment = '/home/bboulbarss/large_experiment_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Y6u-XfvGy0Hd"
   },
   "outputs": [],
   "source": [
    "def experiment(seed, model_name, dataset, split, model, processor):\n",
    "    # Define the CSV file name\n",
    "    now = datetime.datetime.now(ZoneInfo(\"Europe/Amsterdam\"))\n",
    "    formatted_time = now.strftime('%d-%m-%Y_%H-%M-%S')\n",
    "    filename = f\"{snellius_path_experiment}/{dataset}/{model_name}/output_{formatted_time}_{model_name}_seed_{seed}_{split}.csv\"\n",
    "\n",
    "    if model_name == 'idefics' or model_name == 'ovis' or model_name == 'idefics-ft':\n",
    "        full_list = [\"image_path\", \"true_label\", \"completed_prompt\"]\n",
    "    else:\n",
    "        base_list = [\"image_path\", \"true_label\", \"predicted_label\", \"is_correct\"]\n",
    "        dynamic_list = [item for i in range(1, K+2) for item in (f\"label{i}\", f\"prob{i}\")]\n",
    "        full_list = base_list + dynamic_list\n",
    "\n",
    "    # Create and write the header (only once, at the start)\n",
    "    with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(full_list)\n",
    "\n",
    "    # Initialize variables for evaluation\n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "\n",
    "    directory = f'{snellius_path_data}/{dataset}/{split}' \n",
    "    # I changed all the names of the directories manually to lower case for consistency\n",
    "\n",
    "    directories = os.listdir(directory)\n",
    "    total_images_count, _ = count_files_in_subdirs(directory, dataset)\n",
    "    print(f\"Total images: {total_images_count}\")\n",
    "\n",
    "    # Filter out hidden directories before the loop\n",
    "    directories = [d for d in directories if not d.startswith('.')]\n",
    "    print(f\"Total directories: {len(directories)}\")\n",
    "    print(f\"Images per directory: {int(total_images_count/len(directories))}\")\n",
    "\n",
    "    # Iterate over each directory with a progress bar\n",
    "    for f in tqdm(directories, desc=\"Processing directories\"):\n",
    "        if dataset == 'single_object':\n",
    "            # f = 'blue cube' for example\n",
    "            true_label = f'A photo of a {f}'\n",
    "\n",
    "        elif dataset == 'two_object':\n",
    "            parts = f.split('_')\n",
    "            true_label = f\"{parts[0]} {parts[1]}\"\n",
    "            filler_label = f\"{parts[2]} {parts[3]}\"\n",
    "            f += f'/{true_label}'\n",
    "            true_label = f'A photo of a {parts[0]} {parts[1]}'\n",
    "            filler_label = f'A photo of a {parts[2]} {parts[3]}'\n",
    "\n",
    "        elif dataset == 'relational':\n",
    "            parts = f.split('_')\n",
    "            directory_name = f'{parts[0]} {parts[1]} {parts[2]}'\n",
    "            f += f'/{directory_name}'\n",
    "            true_label = f\"A photo of a {parts[0]} {parts [1]} of a {parts[2]}\"\n",
    "\n",
    "        dir = f'{snellius_path_data}/{dataset}/{split}/{f}'\n",
    "        images_paths = os.listdir(dir)\n",
    "\n",
    "        for image in images_paths:\n",
    "            total_images += 1\n",
    "            full_image_path = dir + '/' + image\n",
    "\n",
    "            # Generate distractor labels\n",
    "            if dataset == 'two_object':\n",
    "                distractor_labels = function_map[dataset]((true_label, filler_label))\n",
    "            else:\n",
    "                distractor_labels = function_map[dataset](true_label)\n",
    "\n",
    "            # Combine true label and distractors\n",
    "            all_labels = [true_label] + distractor_labels\n",
    "\n",
    "            # Predict the label\n",
    "            predicted_label, label_probs = function_map[model_name](full_image_path, all_labels, model, processor)\n",
    "\n",
    "            if model_name == 'idefics' or model_name == 'ovis' or model_name == 'idefics-ft':\n",
    "                # Define print functions\n",
    "                def print_details():\n",
    "                    print(f\"Image: {full_image_path}\")\n",
    "                    print(f\"True Label: {true_label}\")\n",
    "                    print(f\"Completed Prompt: {predicted_label}\")\n",
    "                    print(\"-\" * 40)\n",
    "\n",
    "                # Single toggle for all printing\n",
    "                if PRINT_ALL:\n",
    "                    print_details()\n",
    "\n",
    "                # Gather all the data for logging\n",
    "                data = [full_image_path, true_label, predicted_label]\n",
    "\n",
    "                with open(filename, mode=\"a\", newline=\"\") as file:  # \"a\" mode appends data\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(data)\n",
    "\n",
    "            else:\n",
    "                # Define print functions\n",
    "                def print_details():\n",
    "                    print(f\"\\nImage: {full_image_path}\")\n",
    "                    for label, prob in zip(all_labels, label_probs[0]):\n",
    "                        print(f\"Label: {label}, Score: {prob:.4f}\")\n",
    "                    print(f\"Predicted Label: {predicted_label}\")\n",
    "                    print(f\"Correct Prediction: {'Yes' if is_correct else 'No'}\")\n",
    "                    print(\"-\" * 40)\n",
    "\n",
    "                # Check if the prediction is correct\n",
    "                is_correct = predicted_label == true_label\n",
    "                if is_correct:\n",
    "                    total_correct += 1\n",
    "\n",
    "                # Single toggle for all printing\n",
    "                if PRINT_ALL:\n",
    "                    print_details()\n",
    "\n",
    "                # Gather all the data for logging\n",
    "                data = [full_image_path, true_label, predicted_label, is_correct]\n",
    "\n",
    "                for label, prob in zip(all_labels, label_probs[0]):\n",
    "                    data.append(label)\n",
    "                    data.append(prob)\n",
    "\n",
    "                data = [str(item) for item in data]\n",
    "\n",
    "                with open(filename, mode=\"a\", newline=\"\") as file:  # \"a\" mode appends data\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(data)\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = total_correct / total_images\n",
    "    print()\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"seed: {seed}\")\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UbmvX3wd1rcr"
   },
   "outputs": [],
   "source": [
    "function_map = {\n",
    "    'single_object': create_distractors_single_object,\n",
    "    'two_object': create_distractors_two_object,\n",
    "    'relational': create_distractors_relational,\n",
    "\n",
    "    'clip': clip_predict_label,\n",
    "    'clip-ft': clip_predict_label,\n",
    "    'flava': flava_predict_label,\n",
    "    'flava-ft': flava_predict_label,\n",
    "    'vilt': vilt_predict_label,\n",
    "    'vilt-ft': vilt_predict_label,\n",
    "    'idefics': idefics_predict_label,\n",
    "    'idefics-ft': idefics_predict_label,\n",
    "    'ovis': ovis_predict_label\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1899992050a64173a95382cd2a35ce55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 400\n",
      "Total directories: 8\n",
      "Images per directory: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories: 100%|██████████| 8/8 [03:13<00:00, 24.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy: 0.0000\n",
      "Model: idefics-ft\n",
      "Dataset: relational\n",
      "seed: 11\n",
      "-------------------------------------------\n",
      "Total images: 400\n",
      "Total directories: 8\n",
      "Images per directory: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories: 100%|██████████| 8/8 [03:12<00:00, 24.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy: 0.0000\n",
      "Model: idefics-ft\n",
      "Dataset: relational\n",
      "seed: 21\n",
      "-------------------------------------------\n",
      "Total images: 400\n",
      "Total directories: 8\n",
      "Images per directory: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories: 100%|██████████| 8/8 [03:13<00:00, 24.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy: 0.0000\n",
      "Model: idefics-ft\n",
      "Dataset: relational\n",
      "seed: 31\n",
      "-------------------------------------------\n",
      "Total images: 400\n",
      "Total directories: 8\n",
      "Images per directory: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories: 100%|██████████| 8/8 [03:12<00:00, 24.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy: 0.0000\n",
      "Model: idefics-ft\n",
      "Dataset: relational\n",
      "seed: 41\n",
      "-------------------------------------------\n",
      "Total images: 400\n",
      "Total directories: 8\n",
      "Images per directory: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories: 100%|██████████| 8/8 [03:12<00:00, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy: 0.0000\n",
      "Model: idefics-ft\n",
      "Dataset: relational\n",
      "seed: 51\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################### --- EXPERIMENT SETTINGS --- ######################\n",
    "split = 'ood_test'       # Options: 'train', 'id_test', 'ood_test', 'id_val', 'ood_val'\n",
    "model_name = 'vilt-ft'   # Options: 'clip', 'flava', 'vilt', 'idefics' -- 'clip-ft', 'flava-ft', 'vilt-ft', 'idefics-ft'\n",
    "K = 4                    # Number of distractor labels for each experiment (e.g. if K=3, model will get 4 possible labels. 2 of the K labels will always be hard distractors.)\n",
    "PRINT_ALL = False        # Whether to print model prediction details for each image\n",
    "########################################################################\n",
    "\n",
    "if model_name == 'clip':\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "elif model_name == 'flava':\n",
    "    from transformers import FlavaForPreTraining, FlavaProcessor\n",
    "    model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\").to(device)\n",
    "    processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "elif model_name == 'vilt':\n",
    "    from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n",
    "    model = ViltForImageAndTextRetrieval.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\").to(device)\n",
    "    processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n",
    "\n",
    "elif model_name == 'idefics':\n",
    "    from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig\n",
    "    model = IdeficsForVisionText2Text.from_pretrained('HuggingFaceM4/idefics-9b-instruct').to(device)\n",
    "    processor = AutoProcessor.from_pretrained('HuggingFaceM4/idefics-9b-instruct')\n",
    "    # model = IdeficsForVisionText2Text.from_pretrained(\"HuggingFaceM4/tiny-random-idefics\", torch_dtype=torch.bfloat16).to(device)\n",
    "    # processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/tiny-random-idefics\")\n",
    "\n",
    "elif model_name == 'ovis':\n",
    "    from transformers import AutoModelForCausalLM, AutoConfig\n",
    "    # Load and modify the configuration\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        \"AIDC-AI/Ovis2-8B\",\n",
    "        trust_remote_code=True)\n",
    "    \n",
    "    config.llm_attn_implementation = \"eager\"  # Override to use eager attention\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"AIDC-AI/Ovis2-8B\",\n",
    "        config=config,  # Pass the modified config\n",
    "        # torch_dtype=torch.bfloat16,\n",
    "        multimodal_max_length=32768,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\"  # Explicitly set for safety\n",
    "    ).cuda()\n",
    "    \n",
    "    # Get tokenizers from the model\n",
    "    text_tokenizer = model.get_text_tokenizer()\n",
    "    visual_tokenizer = model.get_visual_tokenizer()\n",
    "    \n",
    "    processor = text_tokenizer, visual_tokenizer\n",
    "\n",
    "\n",
    "elif model_name == 'clip-ft':\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # single object\n",
    "    peft_model_path = '/home/bboulbarss/finetuned_models/clip/clip_lora_best_single_object_42_20250523_104223_(8, 16)_32_1e-06'\n",
    "    peft_processor_path = '/home/bboulbarss/finetuned_models/clip/clip_processor_best_single_object_42_20250523_104223_(8, 16)_32_1e-06'\n",
    "\n",
    "    # two object\n",
    "    #peft_model_path = '/home/bboulbarss/finetuned_models/clip/clip_lora_best_two_object_42_20250505_140611_(8, 16)_32_1e-06'\n",
    "    #peft_processor_path = '/home/bboulbarss/finetuned_models/clip/clip_processor_best_two_object_42_20250505_140611_(8, 16)_32_1e-06'\n",
    "    \n",
    "    # relational\n",
    "    #peft_model_path = '/home/bboulbarss/finetuned_models/clip/clip_lora_best_relational_42_20250505_140038_(8, 16)_32_1e-05'\n",
    "    #peft_processor_path = '/home/bboulbarss/finetuned_models/clip/clip_processor_best_relational_42_20250505_140038_(8, 16)_32_1e-05'\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, peft_model_path, is_trainable=False)\n",
    "    processor = CLIPProcessor.from_pretrained(peft_processor_path)\n",
    "\n",
    "elif model_name == 'flava-ft':\n",
    "    from transformers import FlavaForPreTraining, FlavaProcessor\n",
    "    model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\").to(device)\n",
    "    processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "    # single object\n",
    "    peft_model_path = '/home/bboulbarss/finetuned_models/flava/flava_lora_best_single_object_42_20250523_122954_(8, 16)_32_1e-05'\n",
    "    peft_processor_path = '/home/bboulbarss/finetuned_models/flava/flava_processor_best_single_object_42_20250523_122954_(8, 16)_32_1e-05'\n",
    "\n",
    "    # two object\n",
    "    #peft_model_path = '/home/bboulbarss/finetuned_models/flava/flava_lora_best_two_object_42_20250506_081726_(8, 16)_32_1e-06'\n",
    "    #peft_processor_path = '/home/bboulbarss/finetuned_models/flava/flava_processor_best_two_object_42_20250506_081726_(8, 16)_32_1e-06'\n",
    "\n",
    "    # relational\n",
    "    #peft_model_path = '/home/bboulbarss/finetuned_models/flava/flava_lora_best_relational_42_20250505_143920_(8, 16)_32_1e-06'\n",
    "    #peft_processor_path = '/home/bboulbarss/finetuned_models/flava/flava_processor_best_relational_42_20250505_143920_(8, 16)_32_1e-06'\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, peft_model_path, is_trainable=False)\n",
    "    processor = FlavaProcessor.from_pretrained(peft_processor_path)\n",
    "    \n",
    "elif model_name == 'vilt-ft':\n",
    "    from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n",
    "    model = ViltForImageAndTextRetrieval.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\").to(device)\n",
    "    processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n",
    "\n",
    "    # single object\n",
    "    peft_model_path = '/home/bboulbarss/finetuned_models/vilt/vilt_lora_best_single_object_42_20250523_112846_(16, 32)_32_1e-05'\n",
    "    peft_processor_path = '/home/bboulbarss/finetuned_models/vilt/vilt_processor_best_single_object_42_20250523_112846_(16, 32)_32_1e-05'\n",
    "\n",
    "    # two object\n",
    "    #peft_model_path = '/home/bboulbarss/finetuned_models/vilt/vilt_lora_best_two_object_42_20250507_104656_(16, 32)_32_1e-05'\n",
    "    #peft_processor_path = '/home/bboulbarss/finetuned_models/vilt/vilt_processor_best_two_object_42_20250507_104656_(16, 32)_32_1e-05'\n",
    "\n",
    "    # relational\n",
    "    #peft_model_path = '/home/bboulbarss/finetuned_models/vilt//vilt_lora_best_relational_42_20250505_173458_(8, 16)_32_1e-06'\n",
    "    #peft_processor_path = '/home/bboulbarss/finetuned_models/vilt/vilt_processor_best_relational_42_20250505_173458_(8, 16)_32_1e-06'\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, peft_model_path, is_trainable=False)\n",
    "    processor = ViltProcessor.from_pretrained(peft_processor_path)\n",
    "    \n",
    "elif model_name == 'idefics-ft':\n",
    "    from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig\n",
    "    model = IdeficsForVisionText2Text.from_pretrained('HuggingFaceM4/idefics-9b-instruct').to(device)\n",
    "    processor = AutoProcessor.from_pretrained('HuggingFaceM4/idefics-9b-instruct')\n",
    "\n",
    "    # single object\n",
    "    #peft_model_path = '/home/bboulbarss/finetuned_models/idefics/idefics_lora_best_single_object_42_20250525_144228_smalldataset_(16, 32)_1e-05'\n",
    "    #peft_processor_path = '/home/bboulbarss/finetuned_models/idefics/idefics_processor_best_single_object_42_20250525_144228_smalldataset_(16, 32)_1e-05'\n",
    "\n",
    "    # relational\n",
    "    peft_model_path = '/home/bboulbarss/finetuned_models/idefics/idefics_lora_best_relational_42_20250525_160425_smalldataset_(16, 32)_1e-05'\n",
    "    peft_processor_path = '/home/bboulbarss/finetuned_models/idefics/idefics_processor_best_relational_42_20250525_160425_smalldataset_(16, 32)_1e-05'\n",
    "\n",
    "    # two object\n",
    "    #peft_model_path = '/home/bboulbarss/finetuned_models/idefics/idefics_lora_best_two_object_42_20250524_165224_smalldataset_(16, 32)_1e-05'\n",
    "    #peft_processor_path = '/home/bboulbarss/finetuned_models/idefics/idefics_processor_best_two_object_42_20250524_165224_smalldataset_(16, 32)_1e-05'\n",
    "    \n",
    "    model = PeftModel.from_pretrained(model, peft_model_path, is_trainable=False)\n",
    "    processor = AutoProcessor.from_pretrained(peft_processor_path)\n",
    "\n",
    "\n",
    "# Define datasets and seeds for experiments\n",
    "datasets = ['relational'] # 'single_object', 'two_object', 'relational'\n",
    "seeds = [11, 21, 31, 41, 51]\n",
    "\n",
    "# Run experiments for each dataset and seed combination\n",
    "for dataset in datasets:\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        experiment(seed, model_name, dataset, split, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
