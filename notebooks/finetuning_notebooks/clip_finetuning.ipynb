{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchvision\n",
      "Version: 0.22.0\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: /home/bboulbarss/.local/lib/python3.11/site-packages\n",
      "Requires: numpy, pillow, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: peft\n",
      "Version: 0.15.2\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/bboulbarss/.local/lib/python3.11/site-packages\n",
      "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mBo6pWU6VM9q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a-1xCr8iVW-L"
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C5u6EwAhVZyZ"
   },
   "outputs": [],
   "source": [
    "def create_distractors_single_object(true_label):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    colors = ['blue', 'brown', 'cyan', 'gray', 'green', 'purple', 'red', 'yellow']\n",
    "\n",
    "    all_labels = [f\"A photo of a {color} {shape}\" for shape in shapes for color in colors]\n",
    "    all_labels.remove(true_label)\n",
    "\n",
    "    random_labels = random.sample(all_labels, k=4)\n",
    "\n",
    "    return random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T-8rRTYAVanb"
   },
   "outputs": [],
   "source": [
    "def create_distractors_two_object(true_labels):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    colors = ['blue', 'brown', 'cyan', 'gray', 'green', 'purple', 'red', 'yellow']\n",
    "\n",
    "    _, _, _, _, color1, shape1 = true_labels[0].split()\n",
    "    _, _, _, _, color2, shape2 = true_labels[1].split()\n",
    "\n",
    "    hard_distractors = [f\"A photo of a {color1} {shape2}\", f\"A photo of a {color2} {shape1}\"]\n",
    "\n",
    "    exclude = set(true_labels + tuple(hard_distractors))\n",
    "    all_labels = [f\"A photo of a {color} {shape}\" for shape in shapes for color in colors if f\"A photo of a {color} {shape}\" not in exclude]\n",
    "    random_labels = random.sample(all_labels, k=4-len(hard_distractors))\n",
    "\n",
    "    return hard_distractors + random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hbfIKkjmVaYh"
   },
   "outputs": [],
   "source": [
    "def create_distractors_relational(true_label):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    relations = {'right': 'left', 'left': 'right'}\n",
    "\n",
    "    true_parts = true_label.split()\n",
    "    _, _, _, _, true_shape1, true_relation, _, _, true_shape2 = true_parts  # e.g., 'a', 'photo, 'of', 'a', 'sphere', 'right', 'of', 'a', 'cube'\n",
    "\n",
    "    # Define hard distractors\n",
    "    # 1. Shape-swapped: Swap true_shape1 and true_shape2\n",
    "    shape_swapped = f\"A photo of a {true_shape2} {true_relation} of a {true_shape1}\"\n",
    "    # 2. Relation-swapped: Use opposite relation\n",
    "    relation_swapped = f\"A photo of a {true_shape1} {relations[true_relation]} of a {true_shape2}\"\n",
    "\n",
    "    hard_distractors = [shape_swapped, relation_swapped]\n",
    "\n",
    "    # Generate all possible labels\n",
    "    all_labels = [f\"A photo of a {shape} {rel} of a {other_shape}\"\n",
    "                  for shape in shapes\n",
    "                  for rel in relations\n",
    "                  for other_shape in shapes if other_shape != shape]\n",
    "\n",
    "    # Define the inverse label (already true and must be excluded)\n",
    "    inverse_label = f\" A photo of a {true_shape2} {relations[true_relation]} of a {true_shape1}\"\n",
    "\n",
    "    # Filter out true label, inverse label, and ensure hard distractors are unique\n",
    "    exclude = {true_label, inverse_label}\n",
    "    filtered_labels = [label for label in all_labels if label not in exclude]\n",
    "\n",
    "    # Sample random distractors, excluding hard distractors if theyâ€™re already in filtered_labels\n",
    "    random_labels = random.sample([label for label in filtered_labels if label not in hard_distractors], k=4-len(hard_distractors))\n",
    "\n",
    "    return hard_distractors + random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p2CC6CH_Veli"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(ImageFolder):\n",
    "    def __init__(self, root, dataset_name, transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "    def find_classes(self, directory):\n",
    "        classes = [d.name for d in os.scandir(directory) if d.is_dir() and not d.name.startswith('.')]\n",
    "        classes.sort()\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any valid class folders in {directory}\")\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = self.loader(path)  # Load as PIL Image\n",
    "        correct_label = self.classes[target]\n",
    "\n",
    "        if self.dataset_name == 'single_object':\n",
    "            parts = correct_label.split('_')\n",
    "            correct_label = f'A photo of a {parts[0]} {parts[1]}'\n",
    "            labels_list = [correct_label] + create_distractors_single_object(correct_label)\n",
    "        elif self.dataset_name == 'two_object':\n",
    "            parts = correct_label.split('_')\n",
    "            correct_label = f\"A photo of a {parts[0]} {parts[1]}\"\n",
    "            filler_label = f\"A photo of a {parts[2]} {parts[3]}\"\n",
    "            labels_list = [correct_label] + create_distractors_two_object((correct_label, filler_label))\n",
    "        elif self.dataset_name == 'relational':\n",
    "            parts = correct_label.split('_')\n",
    "            correct_label = f\"A photo of a {parts[0]} {parts [1]} of a {parts[2]}\"\n",
    "            labels_list = [correct_label] + create_distractors_relational(correct_label)\n",
    "\n",
    "        random.shuffle(labels_list)\n",
    "        correct_index = labels_list.index(correct_label)\n",
    "\n",
    "        return image, labels_list, correct_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v3h60M4tVgdJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import uuid\n",
    "\n",
    "def train_and_evaluate(dataset_name, base_path='/home/bboulbarss/large_dataset', seed=42):\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Define paths for train, validation, and test splits\n",
    "    train_root = os.path.join(base_path, dataset_name, 'train')\n",
    "    val_ood_root = os.path.join(base_path, dataset_name, 'ood_val')\n",
    "\n",
    "    # Rename directories to replace spaces with underscores\n",
    "    for dir_name in os.listdir(train_root):\n",
    "        if ' ' in dir_name:\n",
    "            new_name = dir_name.replace(' ', '_')\n",
    "            os.rename(\n",
    "                os.path.join(train_root, dir_name),\n",
    "                os.path.join(train_root, new_name)\n",
    "            )\n",
    "    for dir_name in os.listdir(val_ood_root):\n",
    "        if ' ' in dir_name:\n",
    "            new_name = dir_name.replace(' ', '_')\n",
    "            os.rename(\n",
    "                os.path.join(val_ood_root, dir_name),\n",
    "                os.path.join(val_ood_root, new_name)\n",
    "            )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(root=train_root, dataset_name=dataset_name)\n",
    "    val_ood_dataset = CustomDataset(root=val_ood_root, dataset_name=dataset_name)\n",
    "\n",
    "    # Define hyperparameter grid\n",
    "    batch_size = 32\n",
    "    lora_rs = [(8, 16), (16, 32)]\n",
    "    learning_rates = [1e-6, 1e-5]\n",
    "\n",
    "    # Initialize variables to track the best model\n",
    "    best_accuracy = 0.0\n",
    "    best_model_path = None\n",
    "    best_processor_path = None\n",
    "    best_hyperparams = None\n",
    "    best_train_losses = []\n",
    "    timestamp = datetime.now(pytz.timezone('Europe/Amsterdam')).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    # Grid search over hyperparameters\n",
    "    for lora_r, lr in product(lora_rs, learning_rates):\n",
    "        print(f\"\\nTesting hyperparameters: lora_r={lora_r}, lr={lr}\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            collate_fn=lambda x: x, \n",
    "            num_workers=0, \n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_ood_loader = DataLoader(\n",
    "            val_ood_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            collate_fn=lambda x: x, \n",
    "            num_workers=0, \n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Load the base CLIP model and processor\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # Apply LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r[0],\n",
    "            lora_alpha=lora_r[1],\n",
    "            lora_dropout=0.5,\n",
    "            bias=\"none\",\n",
    "            target_modules=[\n",
    "                \"self_attn.q_proj\",\n",
    "                \"self_attn.k_proj\",\n",
    "                \"self_attn.v_proj\",\n",
    "                \"self_attn.out_proj\",\n",
    "                \"mlp.fc1\",\n",
    "                \"mlp.fc2\",\n",
    "                \"visual_projection\",\n",
    "                \"text_projection\"\n",
    "            ]\n",
    "        )\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        for name, param in peft_model.named_parameters():\n",
    "            if \"lora\" not in name.lower():\n",
    "                param.requires_grad = False\n",
    "        model = peft_model\n",
    "        model.to(device)\n",
    "\n",
    "        # Set up optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), \n",
    "            lr=lr, \n",
    "            weight_decay=0.1\n",
    "        )\n",
    "\n",
    "        num_epochs = 15\n",
    "        patience = 3\n",
    "        epochs_no_improve = 0\n",
    "        current_best_accuracy = 0.0\n",
    "        train_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for i, batch in enumerate(train_loader):\n",
    "                images, texts_lists, correct_indices = zip(*batch)\n",
    "                texts = [text for texts_list in texts_lists for text in texts_list]\n",
    "\n",
    "                inputs = processor(\n",
    "                    text=texts,\n",
    "                    images=images,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                ).to(device)\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                logits_per_image = outputs.logits_per_image\n",
    "                B = len(images)\n",
    "                col_indices = (torch.arange(B, device=device) * 5).unsqueeze(1) + torch.arange(5, device=device)\n",
    "                relevant_logits = logits_per_image.gather(1, col_indices)\n",
    "\n",
    "                correct_indices_tensor = torch.tensor(correct_indices, device=device)\n",
    "                loss = nn.CrossEntropyLoss(label_smoothing=0.1)(relevant_logits, correct_indices_tensor)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            train_losses.append(avg_loss)\n",
    "            print(f\"Epoch {epoch+1} completed, Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                print(\"Start OOD Validation Phase\")\n",
    "                for batch in val_ood_loader:\n",
    "                    images, texts_lists, correct_indices = zip(*batch)\n",
    "                    texts = [text for texts_list in texts_lists for text in texts_list]\n",
    "                    inputs = processor(\n",
    "                        text=texts,\n",
    "                        images=images,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True\n",
    "                    ).to(device)\n",
    "\n",
    "                    outputs = model(**inputs)\n",
    "                    logits_per_image = outputs.logits_per_image\n",
    "                    B = len(images)\n",
    "                    col_indices = (torch.arange(B, device=device) * 5).unsqueeze(1) + torch.arange(5, device=device)\n",
    "                    relevant_logits = logits_per_image.gather(1, col_indices)\n",
    "\n",
    "                    preds = relevant_logits.argmax(dim=1)\n",
    "                    correct += (preds == torch.tensor(correct_indices, device=device)).sum().item()\n",
    "                    total += B\n",
    "\n",
    "                accuracy_ood = correct / total\n",
    "                print(f\"Validation (OOD) Accuracy for {dataset_name}: {accuracy_ood:.4f}\")\n",
    "\n",
    "                # Save best model based on OOD validation\n",
    "                if accuracy_ood > current_best_accuracy:\n",
    "                    current_best_accuracy = accuracy_ood\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                # Update global best model if current model is better\n",
    "                if accuracy_ood > best_accuracy:\n",
    "                    best_accuracy = accuracy_ood\n",
    "                    best_hyperparams = {'batch_size': batch_size, 'lora_r': lora_r, 'lr': lr}\n",
    "                    best_train_losses = train_losses.copy()\n",
    "                    model_save_dir = '/home/bboulbarss/finetuned_models/clip'\n",
    "                    os.makedirs(model_save_dir, exist_ok=True)\n",
    "                    best_model_path = os.path.join(\n",
    "                        model_save_dir, \n",
    "                        f'clip_lora_best_{dataset_name}_{seed}_{timestamp}_{lora_r}_{batch_size}_{lr}'\n",
    "                    )\n",
    "                    best_processor_path = os.path.join(\n",
    "                        model_save_dir, \n",
    "                        f'clip_processor_best_{dataset_name}_{seed}_{timestamp}_{lora_r}_{batch_size}_{lr}'\n",
    "                    )\n",
    "                    model.save_pretrained(best_model_path)\n",
    "                    processor.save_pretrained(best_processor_path)\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Plot training loss curve for the best model\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(best_train_losses) + 1), best_train_losses, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Training Loss Curve for Best Model\\nDataset: {dataset_name}, '\n",
    "              f'Hyperparameters: {best_hyperparams}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Training Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(model_save_dir, f'training_loss_curve_{dataset_name}_{seed}_{timestamp}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nBest Model Hyperparameters: {best_hyperparams}\")\n",
    "    print(f\"Best Validation (OOD) Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Best LoRA adapter saved to: {best_model_path}\")\n",
    "    print(f\"Processor saved to: {best_processor_path}\")\n",
    "    print(f\"Training loss curve saved to: {model_save_dir}/training_loss_curve_{dataset_name}_{seed}_{timestamp}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fscMf5jPVqUS",
    "outputId": "b28f0b20-1941-42bb-9018-a35af26176a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on single_object\n",
      "\n",
      "Testing hyperparameters: lora_r=(8, 16), lr=1e-06\n",
      "Epoch 1, Batch 0, Loss: 0.6441\n",
      "Epoch 1, Batch 10, Loss: 0.5829\n",
      "Epoch 1, Batch 20, Loss: 0.6299\n",
      "Epoch 1, Batch 30, Loss: 0.5867\n",
      "Epoch 1, Batch 40, Loss: 0.6083\n",
      "Epoch 1 completed, Average Training Loss: 0.6178\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9475\n",
      "Epoch 2, Batch 0, Loss: 0.5915\n",
      "Epoch 2, Batch 10, Loss: 0.5898\n",
      "Epoch 2, Batch 20, Loss: 0.6178\n",
      "Epoch 2, Batch 30, Loss: 0.5938\n",
      "Epoch 2, Batch 40, Loss: 0.5909\n",
      "Epoch 2 completed, Average Training Loss: 0.5988\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9500\n",
      "Epoch 3, Batch 0, Loss: 0.6021\n",
      "Epoch 3, Batch 10, Loss: 0.6383\n",
      "Epoch 3, Batch 20, Loss: 0.6393\n",
      "Epoch 3, Batch 30, Loss: 0.5927\n",
      "Epoch 3, Batch 40, Loss: 0.5849\n",
      "Epoch 3 completed, Average Training Loss: 0.5814\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9500\n",
      "Epoch 4, Batch 0, Loss: 0.6933\n",
      "Epoch 4, Batch 10, Loss: 0.5968\n",
      "Epoch 4, Batch 20, Loss: 0.5856\n",
      "Epoch 4, Batch 30, Loss: 0.6254\n",
      "Epoch 4, Batch 40, Loss: 0.6816\n",
      "Epoch 4 completed, Average Training Loss: 0.5744\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9500\n",
      "Epoch 5, Batch 0, Loss: 0.5447\n",
      "Epoch 5, Batch 10, Loss: 0.5713\n",
      "Epoch 5, Batch 20, Loss: 0.5209\n",
      "Epoch 5, Batch 30, Loss: 0.5673\n",
      "Epoch 5, Batch 40, Loss: 0.5458\n",
      "Epoch 5 completed, Average Training Loss: 0.5525\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9575\n",
      "Epoch 6, Batch 0, Loss: 0.5529\n",
      "Epoch 6, Batch 10, Loss: 0.5528\n",
      "Epoch 6, Batch 20, Loss: 0.5667\n",
      "Epoch 6, Batch 30, Loss: 0.5299\n",
      "Epoch 6, Batch 40, Loss: 0.5284\n",
      "Epoch 6 completed, Average Training Loss: 0.5337\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9650\n",
      "Epoch 7, Batch 0, Loss: 0.5198\n",
      "Epoch 7, Batch 10, Loss: 0.5340\n",
      "Epoch 7, Batch 20, Loss: 0.5106\n",
      "Epoch 7, Batch 30, Loss: 0.5148\n",
      "Epoch 7, Batch 40, Loss: 0.5456\n",
      "Epoch 7 completed, Average Training Loss: 0.5302\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9550\n",
      "Epoch 8, Batch 0, Loss: 0.5177\n",
      "Epoch 8, Batch 10, Loss: 0.4959\n",
      "Epoch 8, Batch 20, Loss: 0.5961\n",
      "Epoch 8, Batch 30, Loss: 0.4989\n",
      "Epoch 8, Batch 40, Loss: 0.5020\n",
      "Epoch 8 completed, Average Training Loss: 0.5210\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9725\n",
      "Epoch 9, Batch 0, Loss: 0.4984\n",
      "Epoch 9, Batch 10, Loss: 0.5398\n",
      "Epoch 9, Batch 20, Loss: 0.5335\n",
      "Epoch 9, Batch 30, Loss: 0.5087\n",
      "Epoch 9, Batch 40, Loss: 0.5281\n",
      "Epoch 9 completed, Average Training Loss: 0.5124\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9900\n",
      "Epoch 10, Batch 0, Loss: 0.5008\n",
      "Epoch 10, Batch 10, Loss: 0.5053\n",
      "Epoch 10, Batch 20, Loss: 0.5001\n",
      "Epoch 10, Batch 30, Loss: 0.5171\n",
      "Epoch 10, Batch 40, Loss: 0.5063\n",
      "Epoch 10 completed, Average Training Loss: 0.5049\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9700\n",
      "Epoch 11, Batch 0, Loss: 0.5012\n",
      "Epoch 11, Batch 10, Loss: 0.5130\n",
      "Epoch 11, Batch 20, Loss: 0.5100\n",
      "Epoch 11, Batch 30, Loss: 0.4964\n",
      "Epoch 11, Batch 40, Loss: 0.5045\n",
      "Epoch 11 completed, Average Training Loss: 0.5042\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9750\n",
      "Epoch 12, Batch 0, Loss: 0.4961\n",
      "Epoch 12, Batch 10, Loss: 0.4839\n",
      "Epoch 12, Batch 20, Loss: 0.4761\n",
      "Epoch 12, Batch 30, Loss: 0.4841\n",
      "Epoch 12, Batch 40, Loss: 0.5062\n",
      "Epoch 12 completed, Average Training Loss: 0.4997\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9750\n",
      "Early stopping at epoch 12\n",
      "\n",
      "Testing hyperparameters: lora_r=(8, 16), lr=1e-05\n",
      "Epoch 1, Batch 0, Loss: 0.6999\n",
      "Epoch 1, Batch 10, Loss: 0.6553\n",
      "Epoch 1, Batch 20, Loss: 0.5835\n",
      "Epoch 1, Batch 30, Loss: 0.5641\n",
      "Epoch 1, Batch 40, Loss: 0.5168\n",
      "Epoch 1 completed, Average Training Loss: 0.5647\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9650\n",
      "Epoch 2, Batch 0, Loss: 0.4902\n",
      "Epoch 2, Batch 10, Loss: 0.5045\n",
      "Epoch 2, Batch 20, Loss: 0.4913\n",
      "Epoch 2, Batch 30, Loss: 0.4852\n",
      "Epoch 2, Batch 40, Loss: 0.4938\n",
      "Epoch 2 completed, Average Training Loss: 0.4929\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9775\n",
      "Epoch 3, Batch 0, Loss: 0.4732\n",
      "Epoch 3, Batch 10, Loss: 0.4805\n",
      "Epoch 3, Batch 20, Loss: 0.4560\n",
      "Epoch 3, Batch 30, Loss: 0.4658\n",
      "Epoch 3, Batch 40, Loss: 0.5000\n",
      "Epoch 3 completed, Average Training Loss: 0.4777\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9625\n",
      "Epoch 4, Batch 0, Loss: 0.4718\n",
      "Epoch 4, Batch 10, Loss: 0.4584\n",
      "Epoch 4, Batch 20, Loss: 0.4656\n",
      "Epoch 4, Batch 30, Loss: 0.4856\n",
      "Epoch 4, Batch 40, Loss: 0.4674\n",
      "Epoch 4 completed, Average Training Loss: 0.4704\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9650\n",
      "Epoch 5, Batch 0, Loss: 0.4769\n",
      "Epoch 5, Batch 10, Loss: 0.4639\n",
      "Epoch 5, Batch 20, Loss: 0.4547\n",
      "Epoch 5, Batch 30, Loss: 0.4835\n",
      "Epoch 5, Batch 40, Loss: 0.4646\n",
      "Epoch 5 completed, Average Training Loss: 0.4624\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9625\n",
      "Early stopping at epoch 5\n",
      "\n",
      "Testing hyperparameters: lora_r=(16, 32), lr=1e-06\n",
      "Epoch 1, Batch 0, Loss: 0.6581\n",
      "Epoch 1, Batch 10, Loss: 0.6981\n",
      "Epoch 1, Batch 20, Loss: 0.5677\n",
      "Epoch 1, Batch 30, Loss: 0.5573\n",
      "Epoch 1, Batch 40, Loss: 0.6838\n",
      "Epoch 1 completed, Average Training Loss: 0.6082\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9675\n",
      "Epoch 2, Batch 0, Loss: 0.6739\n",
      "Epoch 2, Batch 10, Loss: 0.6123\n",
      "Epoch 2, Batch 20, Loss: 0.5453\n",
      "Epoch 2, Batch 30, Loss: 0.5385\n",
      "Epoch 2, Batch 40, Loss: 0.5409\n",
      "Epoch 2 completed, Average Training Loss: 0.5800\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9625\n",
      "Epoch 3, Batch 0, Loss: 0.5387\n",
      "Epoch 3, Batch 10, Loss: 0.5089\n",
      "Epoch 3, Batch 20, Loss: 0.5459\n",
      "Epoch 3, Batch 30, Loss: 0.5314\n",
      "Epoch 3, Batch 40, Loss: 0.5605\n",
      "Epoch 3 completed, Average Training Loss: 0.5501\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9650\n",
      "Epoch 4, Batch 0, Loss: 0.5484\n",
      "Epoch 4, Batch 10, Loss: 0.5360\n",
      "Epoch 4, Batch 20, Loss: 0.4908\n",
      "Epoch 4, Batch 30, Loss: 0.5261\n",
      "Epoch 4, Batch 40, Loss: 0.5499\n",
      "Epoch 4 completed, Average Training Loss: 0.5295\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9550\n",
      "Early stopping at epoch 4\n",
      "\n",
      "Testing hyperparameters: lora_r=(16, 32), lr=1e-05\n",
      "Epoch 1, Batch 0, Loss: 0.6084\n",
      "Epoch 1, Batch 10, Loss: 0.5355\n",
      "Epoch 1, Batch 20, Loss: 0.5113\n",
      "Epoch 1, Batch 30, Loss: 0.4965\n",
      "Epoch 1, Batch 40, Loss: 0.4843\n",
      "Epoch 1 completed, Average Training Loss: 0.5299\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9800\n",
      "Epoch 2, Batch 0, Loss: 0.4807\n",
      "Epoch 2, Batch 10, Loss: 0.4722\n",
      "Epoch 2, Batch 20, Loss: 0.4868\n",
      "Epoch 2, Batch 30, Loss: 0.4761\n",
      "Epoch 2, Batch 40, Loss: 0.4574\n",
      "Epoch 2 completed, Average Training Loss: 0.4728\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9850\n",
      "Epoch 3, Batch 0, Loss: 0.4750\n",
      "Epoch 3, Batch 10, Loss: 0.4670\n",
      "Epoch 3, Batch 20, Loss: 0.4588\n",
      "Epoch 3, Batch 30, Loss: 0.4694\n",
      "Epoch 3, Batch 40, Loss: 0.4482\n",
      "Epoch 3 completed, Average Training Loss: 0.4620\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9625\n",
      "Epoch 4, Batch 0, Loss: 0.4407\n",
      "Epoch 4, Batch 10, Loss: 0.4461\n",
      "Epoch 4, Batch 20, Loss: 0.4620\n",
      "Epoch 4, Batch 30, Loss: 0.4450\n",
      "Epoch 4, Batch 40, Loss: 0.4494\n",
      "Epoch 4 completed, Average Training Loss: 0.4584\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9775\n",
      "Epoch 5, Batch 0, Loss: 0.4453\n",
      "Epoch 5, Batch 10, Loss: 0.4702\n",
      "Epoch 5, Batch 20, Loss: 0.4416\n",
      "Epoch 5, Batch 30, Loss: 0.4540\n",
      "Epoch 5, Batch 40, Loss: 0.4398\n",
      "Epoch 5 completed, Average Training Loss: 0.4492\n",
      "Start OOD Validation Phase\n",
      "Validation (OOD) Accuracy for single_object: 0.9550\n",
      "Early stopping at epoch 5\n",
      "\n",
      "Best Model Hyperparameters: {'batch_size': 32, 'lora_r': (8, 16), 'lr': 1e-06}\n",
      "Best Validation (OOD) Accuracy: 0.9900\n",
      "Best LoRA adapter saved to: /home/bboulbarss/finetuned_models/clip/clip_lora_best_single_object_42_20250523_104223_(8, 16)_32_1e-06\n",
      "Processor saved to: /home/bboulbarss/finetuned_models/clip/clip_processor_best_single_object_42_20250523_104223_(8, 16)_32_1e-06\n",
      "Training loss curve saved to: /home/bboulbarss/finetuned_models/clip/training_loss_curve_single_object_42_20250523_104223.png\n"
     ]
    }
   ],
   "source": [
    "datasets = ['single_object'] # 'single_object', 'relational', 'two_object'\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nTraining on {dataset}\")\n",
    "    train_and_evaluate(dataset, seed=42)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
