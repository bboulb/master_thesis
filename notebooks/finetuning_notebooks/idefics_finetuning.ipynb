{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhpEfj4DgNl4",
    "outputId": "9922a9e1-6cec-4a8b-bc1c-2aab21f13103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: datasets\n",
      "Version: 3.5.1\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /home/bboulbarss/.local/lib/python3.11/site-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: \n",
      "Name: bitsandbytes\n",
      "Version: 0.45.5\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Tim Dettmers <dettmers@cs.washington.edu>\n",
      "License: MIT License\n",
      "\n",
      "Copyright (c) Facebook, Inc. and its affiliates.\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "Location: /home/bboulbarss/.local/lib/python3.11/site-packages\n",
      "Requires: numpy, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show datasets\n",
    "!pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K4gJLPBSdTpZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from datasets import Dataset, Features, Image as HFImage, Value, Sequence\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainerCallback, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import IdeficsForVisionText2Text, AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mAJlJdBqdcdB"
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LBqgA7wZdfPr"
   },
   "outputs": [],
   "source": [
    "def create_distractors_single_object(true_label):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    colors = ['blue', 'brown', 'cyan', 'gray', 'green', 'purple', 'red', 'yellow']\n",
    "\n",
    "    all_labels = [f\"A photo of a {color} {shape}\" for shape in shapes for color in colors]\n",
    "    all_labels.remove(true_label)\n",
    "\n",
    "    random_labels = random.sample(all_labels, k=4)\n",
    "\n",
    "    return random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rm1IsQnxdgzA"
   },
   "outputs": [],
   "source": [
    "def create_distractors_two_object(true_labels):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    colors = ['blue', 'brown', 'cyan', 'gray', 'green', 'purple', 'red', 'yellow']\n",
    "\n",
    "    _, _, _, _, color1, shape1 = true_labels[0].split()\n",
    "    _, _, _, _, color2, shape2 = true_labels[1].split()\n",
    "\n",
    "    hard_distractors = [f\"A photo of a {color1} {shape2}\", f\"A photo of a {color2} {shape1}\"]\n",
    "\n",
    "    exclude = set(true_labels + tuple(hard_distractors))\n",
    "    all_labels = [f\"A photo of a {color} {shape}\" for shape in shapes for color in colors if f\"A photo of a {color} {shape}\" not in exclude]\n",
    "    random_labels = random.sample(all_labels, k=4-len(hard_distractors))\n",
    "\n",
    "    return hard_distractors + random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1hpVVB14diZl"
   },
   "outputs": [],
   "source": [
    "def create_distractors_relational(true_label):\n",
    "    shapes = ['cube', 'sphere', 'cone', 'cylinder']\n",
    "    relations = {'right': 'left', 'left': 'right'}\n",
    "\n",
    "    true_parts = true_label.split()\n",
    "    _, _, _, _, true_shape1, true_relation, _, _, true_shape2 = true_parts  # e.g., 'a', 'photo, 'of', 'a', 'sphere', 'right', 'of', 'a', 'cube'\n",
    "\n",
    "    # Define hard distractors\n",
    "    # 1. Shape-swapped: Swap true_shape1 and true_shape2\n",
    "    shape_swapped = f\"A photo of a {true_shape2} {true_relation} of a {true_shape1}\"\n",
    "    # 2. Relation-swapped: Use opposite relation\n",
    "    relation_swapped = f\"A photo of a {true_shape1} {relations[true_relation]} of a {true_shape2}\"\n",
    "\n",
    "    hard_distractors = [shape_swapped, relation_swapped]\n",
    "\n",
    "    # Generate all possible labels\n",
    "    all_labels = [f\"A photo of a {shape} {rel} of a {other_shape}\"\n",
    "                  for shape in shapes\n",
    "                  for rel in relations\n",
    "                  for other_shape in shapes if other_shape != shape]\n",
    "\n",
    "    # Define the inverse label (already true and must be excluded)\n",
    "    inverse_label = f\" A photo of a {true_shape2} {relations[true_relation]} of a {true_shape1}\"\n",
    "\n",
    "    # Filter out true label, inverse label, and ensure hard distractors are unique\n",
    "    exclude = {true_label, inverse_label}\n",
    "    filtered_labels = [label for label in all_labels if label not in exclude]\n",
    "\n",
    "    # Sample random distractors, excluding hard distractors if theyâ€™re already in filtered_labels\n",
    "    random_labels = random.sample([label for label in filtered_labels if label not in hard_distractors], k=4-len(hard_distractors))\n",
    "\n",
    "    return hard_distractors + random_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vl_ggmfWdkS8"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(ImageFolder):\n",
    "    def __init__(self, root, dataset_name, transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "    def find_classes(self, directory):\n",
    "        classes = [d.name for d in os.scandir(directory) if d.is_dir() and not d.name.startswith('.')]\n",
    "        classes.sort()\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any valid class folders in {directory}\")\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = self.loader(path)  # Load as PIL Image\n",
    "        correct_label = self.classes[target]\n",
    "\n",
    "        if self.dataset_name == 'single_object':\n",
    "            parts = correct_label.split('_')\n",
    "            correct_label = f'A photo of a {parts[0]} {parts[1]}'\n",
    "            labels_list = [correct_label] + create_distractors_single_object(correct_label)\n",
    "        elif self.dataset_name == 'two_object':\n",
    "            parts = correct_label.split('_')\n",
    "            correct_label = f\"A photo of a {parts[0]} {parts[1]}\"\n",
    "            filler_label = f\"A photo of a {parts[2]} {parts[3]}\"\n",
    "            labels_list = [correct_label] + create_distractors_two_object((correct_label, filler_label))\n",
    "        elif self.dataset_name == 'relational':\n",
    "            parts = correct_label.split('_')\n",
    "            correct_label = f\"A photo of a {parts[0]} {parts [1]} of a {parts[2]}\"\n",
    "            labels_list = [correct_label] + create_distractors_relational(correct_label)\n",
    "\n",
    "        random.shuffle(labels_list)\n",
    "        correct_index = labels_list.index(correct_label)\n",
    "\n",
    "        return image, labels_list, correct_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7IxkfFxlgTky"
   },
   "outputs": [],
   "source": [
    "# Custom data collator to filter unexpected keys\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        batch = {}\n",
    "        # Include image_attention_mask as a valid key\n",
    "        valid_keys = [\"input_ids\", \"attention_mask\", \"pixel_values\", \"image_attention_mask\", \"labels\"]\n",
    "        for key in valid_keys:\n",
    "            if key in examples[0]:\n",
    "                batch[key] = torch.stack([example[key] for example in examples])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J8e9mQL-gYX9"
   },
   "outputs": [],
   "source": [
    "# Define image transform\n",
    "def convert_to_rgb(image):\n",
    "    if image.mode == \"RGB\":\n",
    "        return image\n",
    "    image_rgba = image.convert(\"RGBA\")\n",
    "    background = Image.new(\"RGBA\", image_rgba.size, (255, 255, 255))\n",
    "    alpha_composite = Image.alpha_composite(background, image_rgba)\n",
    "    return alpha_composite.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8lU2tbrfbFp",
    "outputId": "d89b5cf8-210b-4e3f-f6bb-f61d6dcbe27f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 1\n",
      "checkpoint 2\n",
      "checkpoint 3\n"
     ]
    }
   ],
   "source": [
    "# def train_and_evaluate(dataset_name, base_path='/content/drive/MyDrive/thesis_small_dataset', seed=42):\n",
    "dataset_name = 'relational' # 'single_object' 'two_object', 'relational'\n",
    "base_path = '/home/bboulbarss/large_dataset'\n",
    "seed = 42\n",
    "\n",
    "lora_params = (16, 32) #  (8, 16), (16, 32)\n",
    "lr = 1e-6 #  1e-6, 1e-5\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Define paths and dataset\n",
    "train_root = os.path.join(base_path, dataset_name, 'train')\n",
    "val_root = os.path.join(base_path, dataset_name, 'ood_val')\n",
    "\n",
    "for dir_name in os.listdir(train_root):\n",
    "    if ' ' in dir_name:\n",
    "        new_name = dir_name.replace(' ', '_')\n",
    "        os.rename(\n",
    "            os.path.join(train_root, dir_name),\n",
    "            os.path.join(train_root, new_name)\n",
    "        )\n",
    "for dir_name in os.listdir(val_root):\n",
    "    if ' ' in dir_name:\n",
    "        new_name = dir_name.replace(' ', '_')\n",
    "        os.rename(\n",
    "            os.path.join(val_root, dir_name),\n",
    "            os.path.join(val_root, new_name)\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(root=train_root, dataset_name=dataset_name, transform=None)\n",
    "val_dataset = CustomDataset(root=val_root, dataset_name=dataset_name, transform=None)\n",
    "\n",
    "print('checkpoint 1')\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_data = []\n",
    "for image, labels_list, correct_index in train_dataset:\n",
    "    train_data.append({\n",
    "        'image': image,\n",
    "        'labels_list': labels_list,\n",
    "        'correct_index': correct_index\n",
    "    })\n",
    "\n",
    "print('checkpoint 2')\n",
    "\n",
    "val_data = []\n",
    "if val_dataset:\n",
    "    for image, labels_list, correct_index in val_dataset:\n",
    "        val_data.append({\n",
    "            'image': image,\n",
    "            'labels_list': labels_list,\n",
    "            'correct_index': correct_index\n",
    "        })\n",
    "\n",
    "features = Features({\n",
    "    'image': HFImage(),\n",
    "    'labels_list': Sequence(Value('string')),\n",
    "    'correct_index': Value('int64'),\n",
    "})\n",
    "\n",
    "print('checkpoint 3')\n",
    "\n",
    "train_ds = Dataset.from_list(train_data, features=features)\n",
    "eval_ds = Dataset.from_list(val_data, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OK4IlWTE3AQN"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4009e178676a49ee8c3c8d8fcf6e405f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and processor\n",
    "checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n",
    "# checkpoint = \"HuggingFaceM4/tiny-random-idefics\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\"],\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "model = IdeficsForVisionText2Text.from_pretrained(checkpoint, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# LoRA configuration\n",
    "config = LoraConfig(\n",
    "    r=lora_params[0],\n",
    "    lora_alpha=lora_params[1],\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_dropout=0.5,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "71rajeTXzjRH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "{'eval_loss': 2.0477490425109863, 'eval_runtime': 14.3269, 'eval_samples_per_second': 17.45, 'eval_steps_per_second': 4.397, 'epoch': 0.8727272727272727}\n",
      "{'eval_loss': 2.0178675651550293, 'eval_runtime': 14.3207, 'eval_samples_per_second': 17.457, 'eval_steps_per_second': 4.399, 'epoch': 1.8727272727272726}\n",
      "{'eval_loss': 1.979794979095459, 'eval_runtime': 14.3011, 'eval_samples_per_second': 17.481, 'eval_steps_per_second': 4.405, 'epoch': 2.8727272727272726}\n",
      "{'loss': 2.1689, 'grad_norm': 4.454567909240723, 'learning_rate': 8.11111111111111e-07, 'epoch': 3.290909090909091}\n",
      "{'eval_loss': 1.9368896484375, 'eval_runtime': 14.323, 'eval_samples_per_second': 17.454, 'eval_steps_per_second': 4.399, 'epoch': 3.8727272727272726}\n",
      "{'eval_loss': 1.8923307657241821, 'eval_runtime': 14.3025, 'eval_samples_per_second': 17.479, 'eval_steps_per_second': 4.405, 'epoch': 4.872727272727273}\n",
      "{'eval_loss': 1.849433183670044, 'eval_runtime': 14.2562, 'eval_samples_per_second': 17.536, 'eval_steps_per_second': 4.419, 'epoch': 5.872727272727273}\n",
      "{'loss': 2.028, 'grad_norm': 4.010983467102051, 'learning_rate': 5.888888888888889e-07, 'epoch': 6.581818181818182}\n",
      "{'eval_loss': 1.8105590343475342, 'eval_runtime': 14.3537, 'eval_samples_per_second': 17.417, 'eval_steps_per_second': 4.389, 'epoch': 6.872727272727273}\n",
      "{'eval_loss': 1.7741615772247314, 'eval_runtime': 14.3771, 'eval_samples_per_second': 17.389, 'eval_steps_per_second': 4.382, 'epoch': 7.872727272727273}\n",
      "{'eval_loss': 1.744476318359375, 'eval_runtime': 14.3132, 'eval_samples_per_second': 17.466, 'eval_steps_per_second': 4.402, 'epoch': 8.872727272727273}\n",
      "{'loss': 1.8867, 'grad_norm': 3.4894704818725586, 'learning_rate': 3.666666666666666e-07, 'epoch': 9.872727272727273}\n",
      "{'eval_loss': 1.7199128866195679, 'eval_runtime': 14.2614, 'eval_samples_per_second': 17.53, 'eval_steps_per_second': 4.418, 'epoch': 9.872727272727273}\n",
      "{'eval_loss': 1.6989319324493408, 'eval_runtime': 14.3128, 'eval_samples_per_second': 17.467, 'eval_steps_per_second': 4.402, 'epoch': 10.872727272727273}\n",
      "{'eval_loss': 1.6824342012405396, 'eval_runtime': 14.274, 'eval_samples_per_second': 17.514, 'eval_steps_per_second': 4.414, 'epoch': 11.872727272727273}\n",
      "{'eval_loss': 1.670851469039917, 'eval_runtime': 14.4186, 'eval_samples_per_second': 17.339, 'eval_steps_per_second': 4.369, 'epoch': 12.872727272727273}\n",
      "{'loss': 1.8456, 'grad_norm': 3.3295960426330566, 'learning_rate': 1.4444444444444442e-07, 'epoch': 13.290909090909091}\n",
      "{'eval_loss': 1.6624553203582764, 'eval_runtime': 14.2336, 'eval_samples_per_second': 17.564, 'eval_steps_per_second': 4.426, 'epoch': 13.872727272727273}\n",
      "{'eval_loss': 1.6586099863052368, 'eval_runtime': 14.2801, 'eval_samples_per_second': 17.507, 'eval_steps_per_second': 4.412, 'epoch': 14.872727272727273}\n",
      "{'train_runtime': 1036.9745, 'train_samples_per_second': 6.365, 'train_steps_per_second': 0.087, 'train_loss': 1.9544927173190647, 'epoch': 14.872727272727273}\n",
      "Best LoRA adapter saved to: /home/bboulbarss/finetuned_models/idefics/idefics_lora_best_relational_42_20250525_160418_smalldataset_(16, 32)_1e-06\n",
      "Processor saved to: /home/bboulbarss/finetuned_models/idefics/idefics_processor_best_relational_42_20250525_160418_smalldataset_(16, 32)_1e-06\n"
     ]
    }
   ],
   "source": [
    "image_size = processor.image_processor.image_size\n",
    "image_mean = processor.image_processor.image_mean\n",
    "image_std = processor.image_processor.image_std\n",
    "image_transform = transforms.Compose([\n",
    "    convert_to_rgb,\n",
    "    transforms.RandomResizedCrop((image_size, image_size), scale=(0.9, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_mean, std=image_std),\n",
    "])\n",
    "\n",
    "# Define transformation function with debugging\n",
    "def my_ds_transforms(example_batch):\n",
    "    images = example_batch['image']\n",
    "    labels_lists = example_batch['labels_list']\n",
    "    correct_indices = example_batch['correct_index']\n",
    "    prompts = []\n",
    "    for i in range(len(images)):\n",
    "        labels_list = labels_lists[i]\n",
    "        correct_index = correct_indices[i]\n",
    "        letters = ['A', 'B', 'C', 'D', 'E']\n",
    "        correct_letter = letters[correct_index]\n",
    "        choices_text = \"\\n\".join([f\"{letters[j]}. {labels_list[j]}\" for j in range(5)])\n",
    "        text = (f\"Task: Identify the correct label for this image from the following choices:\\n{choices_text}\\n\"\n",
    "                f\"Answer with the letter of the correct choice.\\nAssistant: {correct_letter}\")\n",
    "        prompts.append([\"User:\", images[i], text])\n",
    "\n",
    "    # Process prompts with explicit return_dict\n",
    "    inputs = processor(prompts, transform=image_transform, return_tensors=\"pt\", return_dict=True).to(device)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"]\n",
    "\n",
    "    # Debug: Inspect image_attention_mask\n",
    "    # print(\"Keys in inputs:\", list(inputs.keys()))\n",
    "    if \"image_attention_mask\" in inputs:\n",
    "        # print(\"image_attention_mask value:\", inputs[\"image_attention_mask\"])\n",
    "        if inputs[\"image_attention_mask\"] is None:\n",
    "            # Manually create image_attention_mask\n",
    "            batch_size = inputs[\"input_ids\"].shape[0]\n",
    "            seq_length = inputs[\"input_ids\"].shape[1]\n",
    "            inputs[\"image_attention_mask\"] = torch.ones((batch_size, seq_length), dtype=torch.long, device=device)\n",
    "            print(\"Created manual image_attention_mask with shape:\", inputs[\"image_attention_mask\"].shape)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "train_ds.set_transform(my_ds_transforms)\n",
    "eval_ds.set_transform(my_ds_transforms)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "timestamp = datetime.now(pytz.timezone('Europe/Amsterdam')).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"/home/bboulbarss/finetuned_models/idefics/idefics-{dataset_name}-{timestamp}-TRAININGSAVE\",\n",
    "    learning_rate=lr,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=16,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"epoch\",     # Fixed typo\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=40,\n",
    "    eval_steps=20,\n",
    "    logging_steps=20,\n",
    "    num_train_epochs=15,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    metric_for_best_model=\"eval_loss\",   # Track eval_loss\n",
    "    greater_is_better=False              # Lower eval_loss is better\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize trainer with custom collator and early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=CustomDataCollator(processor),\n",
    "    # compute_metrics=compute_metrics,  # Only needed if using custom metrics\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "print('training')\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA adapter weights and processor for the best model\n",
    "model_save_dir = '/home/bboulbarss/finetuned_models/idefics'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "best_model_path = os.path.join(model_save_dir, f'idefics_lora_best_{dataset_name}_{seed}_{timestamp}_smalldataset_{lora_params}_{lr}')\n",
    "best_processor_path = os.path.join(model_save_dir, f'idefics_processor_best_{dataset_name}_{seed}_{timestamp}_smalldataset_{lora_params}_{lr}')\n",
    "\n",
    "model.save_pretrained(best_model_path)\n",
    "processor.save_pretrained(best_processor_path)\n",
    "\n",
    "print(f\"Best LoRA adapter saved to: {best_model_path}\")\n",
    "print(f\"Processor saved to: {best_processor_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yvPP5O0FhYRd"
   },
   "outputs": [],
   "source": [
    "# datasets = ['two_object'] #'single_object', 'relational',\n",
    "# for dataset in datasets:\n",
    "#     print(f\"\\nTraining on {dataset}\")\n",
    "#     train_and_evaluate(dataset, seed=42)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
