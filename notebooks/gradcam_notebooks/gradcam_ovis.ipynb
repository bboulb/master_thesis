{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial VRAM: 0.00 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de811ff8039749ab82950a6a460b79bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM after model: 16.64 GiB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from torch.amp import autocast\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Initial VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"AIDC-AI/Ovis2-8B\", trust_remote_code=True)\n",
    "config.llm_attn_implementation = \"eager\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AIDC-AI/Ovis2-8B\",\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    multimodal_max_length=32768,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ").cuda()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "text_tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()\n",
    "processor = (text_tokenizer, visual_tokenizer)\n",
    "print(f\"VRAM after model: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7180f3c4961c4e719858df87e25a3bbc",
      "41345f305fd24becb3ca5525834d8e78",
      "615a78b0c901462e99e27440f64080ac",
      "7b552488644c4e8bbc6fbfa82ac982f6",
      "826a49ef29794c6e9e8bfcdc6e7a94fa",
      "d94c265e896f4766b2f03f68491bef5d",
      "8359738435e5498e9ba4909a93a7bf9f",
      "ca81246a6a5e4470934eacad2294c629",
      "33f853f844fa4e4d9e7df7046405304e",
      "412a9570e15541bb822af4fc67efa123",
      "5176bf87008342578fd09eac80102922"
     ]
    },
    "id": "1j553DhU2h6V",
    "outputId": "90bf6662-575c-4fd9-f3d5-c05c86bb317a"
   },
   "outputs": [],
   "source": [
    "def compute_gradcam_ovis(image_path, texts, target_answer, model, processor):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.resize((224, 224))\n",
    "    except Exception as e:\n",
    "        print(f\"Image error: {e}\")\n",
    "        return\n",
    "    images = [image]\n",
    "\n",
    "    shuffled_texts = texts.copy()\n",
    "    random.shuffle(shuffled_texts)\n",
    "    question_string = \"Task: Identify the correct label for this image from the following choices:\\n\" + \"\\n\".join(\n",
    "        [f\"{chr(65+i)}. {text}\" for i, text in enumerate(shuffled_texts)]\n",
    "    ) + \"\\nAnswer with the letter of the correct choice.\"\n",
    "    query = f'<image>\\n{question_string}'\n",
    "\n",
    "    try:\n",
    "        prompt, input_ids, pixel_values = model.preprocess_inputs(query, images, max_partition=12)\n",
    "    except Exception as e:\n",
    "        print(f\"Preprocess error: {e}\")\n",
    "        return\n",
    "    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id)\n",
    "    input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "    pixel_values = pixel_values.to(dtype=torch.bfloat16, device=model.device)\n",
    "    pixel_values = [pixel_values]\n",
    "\n",
    "    print(f\"VRAM after input: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "\n",
    "    target_token_id = text_tokenizer.convert_tokens_to_ids(target_answer)\n",
    "    if target_token_id is None:\n",
    "        print(f\"Invalid answer: {target_answer}\")\n",
    "        return\n",
    "\n",
    "    # Get all layers and set up hooks\n",
    "    blocks = model.visual_tokenizer.backbone.trunk.blocks\n",
    "    num_layers = len(blocks)\n",
    "    print(f\"Total layers to process: {num_layers}\")\n",
    "\n",
    "    # Dictionary to store feature maps for each layer\n",
    "    feature_maps_dict = {}\n",
    "    hooks = []\n",
    "    for layer_idx, block in enumerate(blocks):\n",
    "        def hook_fn(module, input, output, layer_idx=layer_idx):\n",
    "            feature_maps_dict[layer_idx] = output\n",
    "            feature_maps_dict[layer_idx].retain_grad()\n",
    "            print(f\"Layer {layer_idx} - Hooked shape: {feature_maps_dict[layer_idx].shape}, requires_grad: {feature_maps_dict[layer_idx].requires_grad}\")\n",
    "        hook = block.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Forward and backward pass\n",
    "    model.eval()\n",
    "    with torch.enable_grad(), autocast('cuda', dtype=torch.bfloat16):\n",
    "        try:\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            print(f\"VRAM after forward: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "            logits = output.logits[:, -1, :]\n",
    "            target_logit = logits[0, target_token_id].float()\n",
    "            model.zero_grad()\n",
    "            target_logit.backward()\n",
    "            print(f\"VRAM after backward: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Forward/backward error: {e}\")\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "            return\n",
    "\n",
    "    # Compute and save heatmap for each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "        feature_maps = feature_maps_dict.get(layer_idx)\n",
    "        if feature_maps is None:\n",
    "            print(f\"Layer {layer_idx} - Feature maps not captured\")\n",
    "            continue\n",
    "        if feature_maps.grad is None:\n",
    "            print(f\"Layer {layer_idx} - Gradients not captured\")\n",
    "            continue\n",
    "\n",
    "        gradients = feature_maps.grad\n",
    "        print(f\"Layer {layer_idx} - Gradients shape: {gradients.shape}\")\n",
    "        weights = gradients.mean(dim=1, keepdim=True)\n",
    "        heatmap = torch.relu((feature_maps * weights).sum(dim=2))\n",
    "        heatmap = heatmap.view(32, 32)\n",
    "        heatmap = heatmap / (heatmap.max() + 1e-6)\n",
    "\n",
    "        upsample = T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC)\n",
    "        heatmap = upsample(heatmap.to(torch.float32).unsqueeze(0)).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "        heatmap_colored = plt.get_cmap('jet')(heatmap / 255.0)[:, :, :3]\n",
    "        image_np = np.array(image) / 255.0\n",
    "        superimposed_img = heatmap_colored * 0.4 + image_np * 0.6\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(superimposed_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Grad-CAM for Ovis Layer {layer_idx}\")\n",
    "        plt.colorbar(plt.cm.ScalarMappable(cmap='jet'), label='Attention Intensity')\n",
    "        os.makedirs('/home/bboulbarss/gradcam_results/gradcam', exist_ok=True)\n",
    "        plt.savefig(f\"/home/bboulbarss/gradcam_results/gradcam/gradcam_layer_{layer_idx}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Layer {layer_idx} - VRAM after heatmap: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"VRAM after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM after input: 16.64 GiB\n",
      "Total layers to process: 24\n",
      "Layer 0 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 1 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 2 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 3 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 4 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 5 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 6 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 7 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 8 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 9 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 10 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 11 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 12 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 13 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 14 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 15 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 16 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 17 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 18 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 19 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 20 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 21 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 22 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "Layer 23 - Hooked shape: torch.Size([1, 1024, 1536]), requires_grad: True\n",
      "VRAM after forward: 21.51 GiB\n",
      "VRAM after backward: 33.80 GiB\n",
      "Layer 0 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 0 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 1 - Gradients shape: torch.Size([1, 1024, 1536])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/bboulbarss.12373761/ipykernel_1005815/2463373903.py:101: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n",
      "  plt.colorbar(plt.cm.ScalarMappable(cmap='jet'), label='Attention Intensity')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 2 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 2 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 3 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 3 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 4 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 4 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 5 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 5 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 6 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 6 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 7 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 7 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 8 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 8 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 9 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 9 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 10 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 10 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 11 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 11 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 12 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 12 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 13 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 13 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 14 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 14 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 15 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 15 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 16 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 16 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 17 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 17 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 18 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 18 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 19 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 19 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 20 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 20 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 21 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 21 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 22 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 22 - VRAM after heatmap: 33.80 GiB\n",
      "Layer 23 - Gradients shape: torch.Size([1, 1024, 1536])\n",
      "Layer 23 - VRAM after heatmap: 33.80 GiB\n",
      "VRAM after cleanup: 33.80 GiB\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################################\n",
    "\n",
    "########################################\n",
    "############# FINAL IMAGES #############\n",
    "########################################\n",
    "\n",
    "################################################################################################\n",
    "## Image 1 in final_gradcam\n",
    "## Relational\n",
    "#texts = [\n",
    "#    \"A photo of a cylinder left of a cone\",\n",
    "#    \"A photo of a cylinder right of a cone\",\n",
    "#    \"A photo of a cone left of a cylinder\",\n",
    "#    \"A photo of a cube right a cylinder\",\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "#\n",
    "# Two object\n",
    "#texts = [\n",
    "#    \"A photo of a purple cylinder\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a purple cone\",\n",
    "#    \"A photo of a red sphere\",\n",
    "#    \"A photo of a blue cube\"\n",
    "#]\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 2 in final_gradcam\n",
    "# Relational\n",
    "#texts = [\n",
    "#    \"A photo of a cylinder left of a sphere\",\n",
    "#    \"A photo of a cylinder right of a sphere\",\n",
    "#    \"A photo of a sphere left of a cylinder\",\n",
    "#    \"A photo of a cube right a cone\",\n",
    "#    \"A photo of a sphere right of a cone\",]\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "#\n",
    "# Two object\n",
    "#texts = [\n",
    "#    \"A photo of a green sphere\",\n",
    "#    \"A photo of a blue sphere\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a red cone\",\n",
    "#    \"A photo of a purple cube\",\n",
    "#]\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 3 in final_gradcam\n",
    "# Relational\n",
    "#texts = [\n",
    "#    \"A photo of a cone left of a cylinder\",\n",
    "#    \"A photo of a cylinder left of a cone\",\n",
    "#    \"A photo of a cone right of a cylinder\",\n",
    "#    \"A photo of a cone right of a sphere\",\n",
    "#    \"A photo of a cube left of a cylinder\"\n",
    "#]\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cylinder/cone left cylinder/CLEVR_rel_000634.png\"\n",
    "\n",
    "## Two object\n",
    "#texts = [\n",
    "#    \"A photo of a yellow cone\",\n",
    "#    \"A photo of a blue cone\",\n",
    "#    \"A photo of a yellow cylinder\",\n",
    "#    \"A photo of a red sphere\",\n",
    "#    \"A photo of a brown cube\",\n",
    "#]\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cylinder/cone left cylinder/CLEVR_rel_000634.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 5 in final_gradcam\n",
    "# Relational\n",
    "#texts = [\n",
    "#    \"A photo of a cone left of a cube\",\n",
    "#    \"A photo of a cone right of a cube\",\n",
    "#    \"A photo of a cube left of a cone\",\n",
    "#    \"A photo of a sphere right of a cylinder\",\n",
    "#    \"A photo of a cube left of a cylinder\",\n",
    "#]\n",
    "#i#mage_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cube/cone left cube/CLEVR_rel_000466.png\"\n",
    "\n",
    "# Two object\n",
    "texts=[\n",
    "    \"A photo of a purple cube\",\n",
    "    \"A photo of a cyan cube\",\n",
    "    \"A photo of a purple cone\",\n",
    "    \"A photo of a gray sphere\",\n",
    "    \"A photo of a brown cylinder\"\n",
    "]\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/ood_test/cone_left_cube/cone left cube/CLEVR_rel_000466.png\"\n",
    "\n",
    "\n",
    "\n",
    "target_answer = \"A\"\n",
    "compute_gradcam_ovis(image_path, texts, target_answer, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IMAGE 1 #####\n",
    "## RELATIONAL ##\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cube_left_sphere/CLEVR_rel_000065.png\"\n",
    "texts = [\n",
    "    \"A photo of a cube left of a sphere\",\n",
    "    \"A photo of a cube right of a sphere\",\n",
    "    \"A photo of a sphere left of a cube\",\n",
    "    \"A photo of a cube left of a cone\",\n",
    "    \"A photo of a cylinder right of a cube\"\n",
    "]\n",
    "target_label = \"A photo of a cube left of a sphere\"\n",
    "\n",
    "## TWO OBJECT ##\n",
    "texts = [\n",
    "    \"A photo of a gray cube\",\n",
    "    \"A photo of a yellow cube\",\n",
    "    \"A photo of a gray sphere\",\n",
    "    \"A photo of a purple cube\",\n",
    "    \"A photo of a red cone\"\n",
    "]\n",
    "target_label = \"A photo of a grey cube\"\n",
    "\n",
    "\n",
    "##### IMAGE 2 #####\n",
    "## RELATIONAL ##\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000085.png\"\n",
    "texts = [\n",
    "    \"A photo of a cylinder left of a cone\",\n",
    "    \"A photo of a cone left of a cylinder\",\n",
    "    \"A photo of a cylinder right of a cone\",\n",
    "    \"A photo of a cube left of a cone\",\n",
    "    \"A photo of a cylinder right of a cube\"\n",
    "]\n",
    "target_label = \"A photo of a cylinder left of a cone\"\n",
    "\n",
    "## TWO OBJECT ##\n",
    "texts = [\n",
    "    \"A photo of a blue cone\",\n",
    "    \"A photo of a purple cone\",\n",
    "    \"A photo of a blue cylinder\",\n",
    "    \"A photo of a gray cube\",\n",
    "    \"A photo of a red sphere\"\n",
    "]\n",
    "target_label = \"A photo of a blue cone\"\n",
    "\n",
    "\n",
    "##### IMAGE 3 #####\n",
    "## RELATIONAL ##\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cube_right_cylinder/CLEVR_rel_000057.png\"\n",
    "texts = [\n",
    "    \"A photo of a cube right of a cylinder\",\n",
    "    \"A photo of a cube left of a cylinder\",\n",
    "    \"A photo of a cone right of a cylinder\",\n",
    "    \"A photo of a cube left of a cone\",\n",
    "    \"A photo of a cylinder right of a cone\"\n",
    "]\n",
    "target_label = \"A photo of a cube right of a cylinder\"\n",
    "\n",
    "## TWO OBJECT ##\n",
    "texts = [\n",
    "    \"A photo of a blue cylinder\",\n",
    "    \"A photo of a cyan cylinder\",\n",
    "    \"A photo of a blue cube\",\n",
    "    \"A photo of a brown sphere\",\n",
    "    \"A photo of a yellow cube\"\n",
    "]\n",
    "target_label = \"A photo of a blue cylinder\"\n",
    "\n",
    "\n",
    "##### IMAGE 4 #####\n",
    "## RELATIONAL ##\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/sphere_left_cone/CLEVR_rel_000007.png\"\n",
    "texts = [\n",
    "    \"A photo of a sphere left of a cone\",\n",
    "    \"A photo of a sphere right of a cone\",\n",
    "    \"A photo of a cone left of a sphere\",\n",
    "    \"A photo of a cube left of a cone\",\n",
    "    \"A photo of a cylinder right of a cone\"\n",
    "]\n",
    "target_label = \"A photo of a sphere left of a cone\"\n",
    "\n",
    "## TWO OBJECT ##\n",
    "texts = [\n",
    "    \"A photo of a purple cone\",\n",
    "    \"A photo of a blue cone\",\n",
    "    \"A photo of a purple sphere\",\n",
    "    \"A photo of a green cylinder\",\n",
    "    \"A photo of a yellow cube\"\n",
    "]\n",
    "target_label = \"A photo of a purple cone\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "33f853f844fa4e4d9e7df7046405304e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "412a9570e15541bb822af4fc67efa123": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41345f305fd24becb3ca5525834d8e78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d94c265e896f4766b2f03f68491bef5d",
      "placeholder": "​",
      "style": "IPY_MODEL_8359738435e5498e9ba4909a93a7bf9f",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "5176bf87008342578fd09eac80102922": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "615a78b0c901462e99e27440f64080ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca81246a6a5e4470934eacad2294c629",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_33f853f844fa4e4d9e7df7046405304e",
      "value": 4
     }
    },
    "7180f3c4961c4e719858df87e25a3bbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_41345f305fd24becb3ca5525834d8e78",
       "IPY_MODEL_615a78b0c901462e99e27440f64080ac",
       "IPY_MODEL_7b552488644c4e8bbc6fbfa82ac982f6"
      ],
      "layout": "IPY_MODEL_826a49ef29794c6e9e8bfcdc6e7a94fa"
     }
    },
    "7b552488644c4e8bbc6fbfa82ac982f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_412a9570e15541bb822af4fc67efa123",
      "placeholder": "​",
      "style": "IPY_MODEL_5176bf87008342578fd09eac80102922",
      "value": " 4/4 [00:05&lt;00:00,  1.24s/it]"
     }
    },
    "826a49ef29794c6e9e8bfcdc6e7a94fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8359738435e5498e9ba4909a93a7bf9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca81246a6a5e4470934eacad2294c629": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d94c265e896f4766b2f03f68491bef5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
