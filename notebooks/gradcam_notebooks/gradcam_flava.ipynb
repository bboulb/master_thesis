{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c67462-2d79-4b5d-9e8f-27ee6fd49d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlavaForPreTraining(\n",
       "  (flava): FlavaModel(\n",
       "    (text_model): FlavaTextModel(\n",
       "      (embeddings): FlavaTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): FlavaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x FlavaLayer(\n",
       "            (attention): FlavaAttention(\n",
       "              (attention): FlavaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): FlavaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): FlavaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): FlavaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): FlavaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (image_model): FlavaImageModel(\n",
       "      (embeddings): FlavaImageEmbeddings(\n",
       "        (patch_embeddings): PatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): FlavaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x FlavaLayer(\n",
       "            (attention): FlavaAttention(\n",
       "              (attention): FlavaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): FlavaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): FlavaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): FlavaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): FlavaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (multimodal_model): FlavaMultimodalModel(\n",
       "      (encoder): FlavaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x FlavaLayer(\n",
       "            (attention): FlavaAttention(\n",
       "              (attention): FlavaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): FlavaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): FlavaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): FlavaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): FlavaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (image_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (text_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (image_to_mm_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (text_to_mm_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (image_codebook): FlavaImageCodebook(\n",
       "    (blocks): Sequential(\n",
       "      (input): Conv2d(3, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (group_1): FlavaImageCodebookLayerGroup(\n",
       "        (group): Sequential(\n",
       "          (block_1): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block_2): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (group_2): FlavaImageCodebookLayerGroup(\n",
       "        (group): Sequential(\n",
       "          (block_1): FlavaImageCodebookBlock(\n",
       "            (id_path): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block_2): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (group_3): FlavaImageCodebookLayerGroup(\n",
       "        (group): Sequential(\n",
       "          (block_1): FlavaImageCodebookBlock(\n",
       "            (id_path): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block_2): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (group_4): FlavaImageCodebookLayerGroup(\n",
       "        (group): Sequential(\n",
       "          (block_1): FlavaImageCodebookBlock(\n",
       "            (id_path): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block_2): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output): Sequential(\n",
       "        (relu): ReLU()\n",
       "        (conv): Conv2d(2048, 8192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mim_head): FlavaMaskedPredictionHead(\n",
       "    (transform): FlavaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=8192, bias=True)\n",
       "  )\n",
       "  (mlm_head): FlavaMaskedPredictionHead(\n",
       "    (transform): FlavaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       "  (itm_head): FlavaITMHead(\n",
       "    (pooler): FlavaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (mmm_image_head): FlavaMaskedPredictionHead(\n",
       "    (transform): FlavaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=8192, bias=True)\n",
       "  )\n",
       "  (mmm_text_head): FlavaMaskedPredictionHead(\n",
       "    (transform): FlavaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       "  (global_contrastive_head): FlavaGlobalContrastiveHead()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import FlavaProcessor, FlavaForPreTraining\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from torch.amp import autocast\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set environment variables for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load FLAVA model and processor\n",
    "model_name = \"facebook/flava-full\"\n",
    "processor = FlavaProcessor.from_pretrained(model_name)\n",
    "model = FlavaForPreTraining.from_pretrained(model_name).to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517d5e30-26bf-479c-be23-bf42c5f80ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`input_ids_masked` isn't passed which means MLM loss won't be calculated correctlySetting it to `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if you are doing inference on unmasked text...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers in FLAVA Image Encoder: 12\n",
      "Layer 0 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 1 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 2 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 3 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 4 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 5 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 6 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 7 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 8 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 9 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 10 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 11 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 0 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 1 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 2 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 3 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 4 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 5 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 6 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 7 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 8 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 9 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 10 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n",
      "Layer 11 - Hooked shape: torch.Size([5, 197, 768]), requires_grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bboulbarss/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:1044: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive logits shape: torch.Size([5, 5])\n",
      "Target logit: 21.54041862487793\n",
      "Grad hook called for layer 11 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 10 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 9 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 8 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 7 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 6 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 5 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 4 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 3 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 2 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 1 with grad shape: torch.Size([5, 197, 768])\n",
      "Grad hook called for layer 0 with grad shape: torch.Size([5, 197, 768])\n",
      "VRAM after backward: 5.03 GiB\n",
      "Layer 0 - Gradients shape: torch.Size([5, 197, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/bboulbarss.12373455/ipykernel_2996152/3341809740.py:109: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n",
      "  plt.colorbar(plt.cm.ScalarMappable(cmap='jet'), label='Attention Intensity')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 1 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 1 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 2 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 2 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 3 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 3 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 4 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 4 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 5 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 5 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 6 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 6 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 7 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 7 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 8 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 8 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 9 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 9 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 10 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 10 - VRAM after heatmap: 5.03 GiB\n",
      "Layer 11 - Gradients shape: torch.Size([5, 197, 768])\n",
      "Layer 11 - VRAM after heatmap: 5.03 GiB\n",
      "VRAM after cleanup: 5.03 GiB\n"
     ]
    }
   ],
   "source": [
    "def compute_gradcam_flava(image_path, texts, target_label, model, processor):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    images = [image] * len(texts)  # Replicate image to match number of texts\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding='max_length',\n",
    "        max_length=77,\n",
    "        return_codebook_pixels=True,\n",
    "        return_image_mask=True,\n",
    "        return_attention_mask=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Get the target label index\n",
    "    try:\n",
    "        target_label_idx = texts.index(target_label)\n",
    "    except ValueError:\n",
    "        print(f\"Target label '{target_label}' not found in texts\")\n",
    "        return\n",
    "\n",
    "    # Get all layers in the FLAVA image encoder\n",
    "    num_layers = len(model.flava.image_model.encoder.layer)\n",
    "    print(f\"Total layers in FLAVA Image Encoder: {num_layers}\")\n",
    "\n",
    "    # Dictionary to store feature maps for each layer\n",
    "    feature_maps_dict = {}\n",
    "    hooks = []\n",
    "\n",
    "    # Register hooks for all layers\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer = model.flava.image_model.encoder.layer[layer_idx]\n",
    "        def hook_fn(module, input, output, idx=layer_idx):\n",
    "            if isinstance(output, tuple):\n",
    "                feature_maps = output[0]\n",
    "            else:\n",
    "                feature_maps = output\n",
    "            feature_maps_dict[idx] = feature_maps\n",
    "            feature_maps.retain_grad()  # Ensure gradients are retained\n",
    "            def grad_hook(grad):\n",
    "                print(f\"Grad hook called for layer {idx} with grad shape: {grad.shape}\")\n",
    "                feature_maps_dict[idx].grad = grad.clone()\n",
    "            feature_maps.register_hook(grad_hook)\n",
    "            print(f\"Layer {idx} - Hooked shape: {feature_maps.shape}, requires_grad: {feature_maps.requires_grad}\")\n",
    "        hook = layer.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Forward and backward pass\n",
    "    with torch.enable_grad(), autocast('cuda', dtype=torch.float32):\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.contrastive_logits_per_image\n",
    "            print(f\"Contrastive logits shape: {logits_per_image.shape}\")\n",
    "            target_logit = logits_per_image[0, target_label_idx].float()\n",
    "            print(f\"Target logit: {target_logit.item()}\")\n",
    "            model.zero_grad()\n",
    "            target_logit.backward()\n",
    "            print(f\"VRAM after backward: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Forward/backward error: {e}\")\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "            return\n",
    "\n",
    "    # Compute and save heatmap for each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "        feature_maps = feature_maps_dict.get(layer_idx)\n",
    "        if feature_maps is None:\n",
    "            print(f\"Layer {layer_idx} - Feature maps not captured\")\n",
    "            continue\n",
    "        if not hasattr(feature_maps, 'grad') or feature_maps.grad is None:\n",
    "            print(f\"Layer {layer_idx} - Gradients not captured\")\n",
    "            continue\n",
    "\n",
    "        gradients = feature_maps.grad\n",
    "        print(f\"Layer {layer_idx} - Gradients shape: {gradients.shape}\")\n",
    "\n",
    "        # Use feature maps and gradients for the first image\n",
    "        feature_maps_first = feature_maps[0, :, :]  # (197, 768)\n",
    "        gradients_first = gradients[0, :, :]  # (197, 768)\n",
    "\n",
    "        # Ignore CLS token\n",
    "        feature_maps_patches = feature_maps_first[1:, :]  # (196, 768)\n",
    "        gradients_patches = gradients_first[1:, :]  # (196, 768)\n",
    "\n",
    "        # Compute weights\n",
    "        weights = gradients_patches.mean(dim=0, keepdim=True)  # (1, 768)\n",
    "\n",
    "        # Compute heatmap\n",
    "        heatmap = torch.relu((feature_maps_patches * weights).sum(dim=1))  # (196,)\n",
    "        heatmap = heatmap.view(14, 14)  # Reshape to 14x14 grid\n",
    "        heatmap = heatmap / (heatmap.max() + 1e-6)\n",
    "\n",
    "        # Upsample to image size\n",
    "        upsample = T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC)\n",
    "        heatmap = upsample(heatmap.to(torch.float32).unsqueeze(0)).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # Visualize\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "        heatmap_colored = plt.get_cmap('jet')(heatmap / 255.0)[:, :, :3]\n",
    "        image_np = np.array(image.resize((224, 224))) / 255.0\n",
    "        superimposed_img = heatmap_colored * 0.4 + image_np * 0.6\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(superimposed_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Grad-CAM for FLAVA Layer {layer_idx}\")\n",
    "        plt.colorbar(plt.cm.ScalarMappable(cmap='jet'), label='Attention Intensity')\n",
    "        #os.makedirs('/home/bboulbarss/gradcam/flava/flava_gradcam_', exist_ok=True)\n",
    "        plt.savefig(f\"/home/bboulbarss/gradcam_results/image1/flava-rel/thesis_flava_gradcam_rel_1_predicted_label/gradcam_flava_layer_{layer_idx}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Layer {layer_idx} - VRAM after heatmap: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "\n",
    "    # Clean up\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"VRAM after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "########################################\n",
    "############# FINAL IMAGES #############\n",
    "########################################\n",
    "\n",
    "################################################################################################\n",
    "## Image 1 in final_gradcam\n",
    "## Relational, correct label\n",
    "texts = [\n",
    "    \"A photo of a cylinder left of a cone\",\n",
    "    \"A photo of a cylinder right of a cone\",\n",
    "    \"A photo of a cone left of a cylinder\",\n",
    "    \"A photo of a cube right of a cylinder\",\n",
    "    \"A photo of a sphere right of a cone\",]\n",
    "target_label = \"A photo of a cylinder left of a cone\"\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "\n",
    "## Relational, predicted label\n",
    "texts = [\n",
    "    \"A photo of a cylinder left of a cone\",\n",
    "    \"A photo of a cylinder right of a cone\",\n",
    "    \"A photo of a cone left of a cylinder\",\n",
    "    \"A photo of a cube right of a cylinder\",\n",
    "    \"A photo of a sphere right of a cone\",]\n",
    "target_label = \"A photo of a cube right of a cylinder\"\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "\n",
    "\n",
    "# Two object\n",
    "#texts = [\n",
    "#    \"A photo of a purple cylinder\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a purple cone\",\n",
    "#    \"A photo of a red sphere\",\n",
    "#    \"A photo of a blue cube\"\n",
    "#]\n",
    "#target_label = \"A photo of a purple cylinder\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 2 in final_gradcam\n",
    "# Relational\n",
    "#texts = [\n",
    "#    \"A photo of a cylinder left of a sphere\",\n",
    "#    \"A photo of a cylinder right of a sphere\",\n",
    "#    \"A photo of a sphere left of a cylinder\",\n",
    "#    \"A photo of a cube right a cone\",\n",
    "#    \"A photo of a sphere right of a cone\",]\n",
    "#target_label = \"A photo of a cylinder left of a sphere\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "#\n",
    "# Two object\n",
    "#texts = [\n",
    "#    \"A photo of a green sphere\",\n",
    "#    \"A photo of a blue sphere\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a red cone\",\n",
    "#    \"A photo of a purple cube\",\n",
    "#]\n",
    "#target_label = \"A photo of a green sphere\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "compute_gradcam_flava(image_path, texts, target_label, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d173c56-dfe4-42c1-9452-29f6163677ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### IMAGE 1 #####\n",
    "## RELATIONAL ##\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cube_left_sphere/CLEVR_rel_000065.png\"\n",
    "texts = [\n",
    "    \"A photo of a cube left of a sphere\",\n",
    "    \"A photo of a cube right of a sphere\",\n",
    "    \"A photo of a sphere left of a cube\",\n",
    "    \"A photo of a cube left of a cone\",\n",
    "    \"A photo of a cylinder right of a cube\"\n",
    "]\n",
    "target_label = \"A photo of a cube left of a sphere\"\n",
    "\n",
    "## TWO OBJECT ##\n",
    "texts = [\n",
    "    \"A photo of a gray cube\",\n",
    "    \"A photo of a yellow cube\",\n",
    "    \"A photo of a gray sphere\",\n",
    "    \"A photo of a purple cube\",\n",
    "    \"A photo of a red cone\"\n",
    "]\n",
    "target_label = \"A photo of a grey cube\"\n",
    "\n",
    "\n",
    "##### IMAGE 2 #####\n",
    "## RELATIONAL ##\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000085.png\"\n",
    "texts = [\n",
    "    \"A photo of a cylinder left of a cone\",\n",
    "    \"A photo of a cone left of a cylinder\",\n",
    "    \"A photo of a cylinder right of a cone\",\n",
    "    \"A photo of a cube left of a cone\",\n",
    "    \"A photo of a cylinder right of a cube\"\n",
    "]\n",
    "target_label = \"A photo of a cylinder left of a cone\"\n",
    "\n",
    "## TWO OBJECT ##\n",
    "texts = [\n",
    "    \"A photo of a blue cone\",\n",
    "    \"A photo of a purple cone\",\n",
    "    \"A photo of a blue cylinder\",\n",
    "    \"A photo of a gray cube\",\n",
    "    \"A photo of a red sphere\"\n",
    "]\n",
    "target_label = \"A photo of a blue cone\"\n",
    "\n",
    "\n",
    "##### IMAGE 3 #####\n",
    "## RELATIONAL ##\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cube_right_cylinder/CLEVR_rel_000057.png\"\n",
    "texts = [\n",
    "    \"A photo of a cube right of a cylinder\",\n",
    "    \"A photo of a cube left of a cylinder\",\n",
    "    \"A photo of a cone right of a cylinder\",\n",
    "    \"A photo of a cube left of a cone\",\n",
    "    \"A photo of a cylinder right of a cone\"\n",
    "]\n",
    "target_label = \"A photo of a cube right of a cylinder\"\n",
    "\n",
    "## TWO OBJECT ##\n",
    "texts = [\n",
    "    \"A photo of a blue cylinder\",\n",
    "    \"A photo of a cyan cylinder\",\n",
    "    \"A photo of a blue cube\",\n",
    "    \"A photo of a brown sphere\",\n",
    "    \"A photo of a yellow cube\"\n",
    "]\n",
    "target_label = \"A photo of a blue cylinder\"\n",
    "\n",
    "\n",
    "##### IMAGE 4 #####\n",
    "## RELATIONAL ##\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/sphere_left_cone/CLEVR_rel_000007.png\"\n",
    "texts = [\n",
    "    \"A photo of a sphere left of a cone\",\n",
    "    \"A photo of a sphere right of a cone\",\n",
    "    \"A photo of a cone left of a sphere\",\n",
    "    \"A photo of a cube left of a cone\",\n",
    "    \"A photo of a cylinder right of a cone\"\n",
    "]\n",
    "target_label = \"A photo of a sphere left of a cone\"\n",
    "\n",
    "## TWO OBJECT ##\n",
    "texts = [\n",
    "    \"A photo of a purple cone\",\n",
    "    \"A photo of a blue cone\",\n",
    "    \"A photo of a purple sphere\",\n",
    "    \"A photo of a green cylinder\",\n",
    "    \"A photo of a yellow cube\"\n",
    "]\n",
    "target_label = \"A photo of a purple cone\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
