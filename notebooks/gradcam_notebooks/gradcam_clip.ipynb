{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4693c211-d856-4a2a-a1a0-aced3ece2ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from torch.amp import autocast\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set environment variables for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name).to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b027a6-a434-4d56-9a39-49da923da564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers in CLIP Vision Transformer: 12\n",
      "Layer 0 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 1 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 2 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 3 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 4 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 5 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 6 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 7 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 8 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 9 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 10 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "Layer 11 - Hooked shape: torch.Size([1, 50, 768]), requires_grad: True\n",
      "VRAM after backward: 1.19 GiB\n",
      "Layer 0 - Gradients shape: torch.Size([1, 50, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/bboulbarss.12373455/ipykernel_2980266/1656700780.py:80: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n",
      "  plt.colorbar(plt.cm.ScalarMappable(cmap='jet'), label='Attention Intensity')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 1 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 1 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 2 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 2 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 3 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 3 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 4 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 4 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 5 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 5 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 6 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 6 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 7 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 7 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 8 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 8 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 9 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 9 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 10 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 10 - VRAM after heatmap: 1.19 GiB\n",
      "Layer 11 - Gradients shape: torch.Size([1, 50, 768])\n",
      "Layer 11 - VRAM after heatmap: 1.19 GiB\n",
      "VRAM after cleanup: 1.19 GiB\n"
     ]
    }
   ],
   "source": [
    "def compute_gradcam_clip(image_path, texts, target_label, model, processor):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    # Get the target label index\n",
    "    try:\n",
    "        target_label_idx = texts.index(target_label)\n",
    "    except ValueError:\n",
    "        print(f\"Target label '{target_label}' not found in texts\")\n",
    "        return\n",
    "\n",
    "    # Get all Vision Transformer layers\n",
    "    num_layers = len(model.vision_model.encoder.layers)\n",
    "    print(f\"Total layers in CLIP Vision Transformer: {num_layers}\")\n",
    "\n",
    "    # Dictionary to store feature maps for each layer\n",
    "    feature_maps_dict = {}\n",
    "    hooks = []\n",
    "\n",
    "    # Register hooks for all layers\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer = model.vision_model.encoder.layers[layer_idx]\n",
    "        def hook_fn(module, input, output, idx=layer_idx):\n",
    "            # Output is a tuple; take the first element (hidden states)\n",
    "            feature_maps = output[0]\n",
    "            feature_maps_dict[idx] = feature_maps\n",
    "            feature_maps_dict[idx].retain_grad()\n",
    "            print(f\"Layer {idx} - Hooked shape: {feature_maps.shape}, requires_grad: {feature_maps.requires_grad}\")\n",
    "        hook = layer.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Forward and backward pass\n",
    "    with torch.enable_grad(), autocast('cuda', dtype=torch.float32):\n",
    "        try:\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            target_logit = logits_per_image[0, target_label_idx].float()\n",
    "            model.zero_grad()\n",
    "            target_logit.backward()\n",
    "            print(f\"VRAM after backward: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Forward/backward error: {e}\")\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "            return\n",
    "\n",
    "    # Compute and save heatmap for each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "        feature_maps = feature_maps_dict.get(layer_idx)\n",
    "        if feature_maps is None:\n",
    "            print(f\"Layer {layer_idx} - Feature maps not captured\")\n",
    "            continue\n",
    "        if feature_maps.grad is None:\n",
    "            print(f\"Layer {layer_idx} - Gradients not captured\")\n",
    "            continue\n",
    "\n",
    "        gradients = feature_maps.grad\n",
    "        print(f\"Layer {layer_idx} - Gradients shape: {gradients.shape}\")\n",
    "        # Shape: (batch_size, num_patches + 1, hidden_size), ignore CLS token\n",
    "        weights = gradients[:, 1:, :].mean(dim=1, keepdim=True)  # Average over patches\n",
    "        heatmap = torch.relu((feature_maps[:, 1:, :] * weights).sum(dim=2))\n",
    "        # Reshape to 7x7 grid (49 patches = 7x7 for 224x224 image with patch size 32)\n",
    "        heatmap = heatmap.view(7, 7)\n",
    "        heatmap = heatmap / (heatmap.max() + 1e-6)\n",
    "\n",
    "        # Upsample to image size\n",
    "        upsample = T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC)\n",
    "        heatmap = upsample(heatmap.to(torch.float32).unsqueeze(0)).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # Visualize\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "        heatmap_colored = plt.get_cmap('jet')(heatmap / 255.0)[:, :, :3]\n",
    "        image_np = np.array(image.resize((224, 224))) / 255.0\n",
    "        superimposed_img = heatmap_colored * 0.4 + image_np * 0.6\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(superimposed_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Grad-CAM for CLIP Layer {layer_idx}\")\n",
    "        plt.colorbar(plt.cm.ScalarMappable(cmap='jet'), label='Attention Intensity')\n",
    "        #os.makedirs('/home/bboulbarss/gradcam/clip/clip_gradcam_', exist_ok=True)\n",
    "        plt.savefig(f\"/home/bboulbarss/gradcam_results/image2/clip-rel/thesis_clip_gradcam_rel_2_predicted_label/gradcam_clip_layer_{layer_idx}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Layer {layer_idx} - VRAM after heatmap: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "\n",
    "    # Clean up\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"VRAM after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "###### IMAGE 1 #####\n",
    "### RELATIONAL ##\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cube_left_sphere/CLEVR_rel_000065.png\"\n",
    "#texts = [\n",
    "#    \"A photo of a cube left of a sphere\",\n",
    "#    \"A photo of a cube right of a sphere\",\n",
    "#    \"A photo of a sphere left of a cube\",\n",
    "#    \"A photo of a cube left of a cone\",\n",
    "#    \"A photo of a cylinder right of a cube\"\n",
    "#]\n",
    "#target_label = \"A photo of a cube left of a sphere\"\n",
    "#\n",
    "### TWO OBJECT ##\n",
    "#texts = [\n",
    "#    \"A photo of a gray cube\",\n",
    "#    \"A photo of a yellow cube\",\n",
    "#    \"A photo of a gray sphere\",\n",
    "#    \"A photo of a purple cube\",\n",
    "#    \"A photo of a red cone\"\n",
    "#]\n",
    "#target_label = \"A photo of a gray cube\"\n",
    "#\n",
    "#\n",
    "###### IMAGE 2 #####\n",
    "### RELATIONAL ##\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000085.png\"\n",
    "#texts = [\n",
    "#    \"A photo of a cylinder left of a cone\",\n",
    "#    \"A photo of a cone left of a cylinder\",\n",
    "#    \"A photo of a cylinder right of a cone\",\n",
    "#    \"A photo of a cube left of a cone\",\n",
    "#    \"A photo of a cylinder right of a cube\"\n",
    "#]\n",
    "#target_label = \"A photo of a cylinder left of a cone\"\n",
    "#\n",
    "### TWO OBJECT ##\n",
    "#texts = [\n",
    "#    \"A photo of a blue cone\",\n",
    "#    \"A photo of a purple cone\",\n",
    "#    \"A photo of a blue cylinder\",\n",
    "#    \"A photo of a gray cube\",\n",
    "#    \"A photo of a red sphere\"\n",
    "#]\n",
    "#target_label = \"A photo of a blue cone\"\n",
    "#\n",
    "#\n",
    "###### IMAGE 3 #####\n",
    "### RELATIONAL ##\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cube_right_cylinder/CLEVR_rel_000057.png\"\n",
    "#texts = [\n",
    "#    \"A photo of a cube right of a cylinder\",\n",
    "#    \"A photo of a cube left of a cylinder\",\n",
    "#    \"A photo of a cone right of a cylinder\",\n",
    "#    \"A photo of a cube left of a cone\",\n",
    "#    \"A photo of a cylinder right of a cone\"\n",
    "#]\n",
    "#target_label = \"A photo of a cube right of a cylinder\"\n",
    "#\n",
    "### TWO OBJECT ##\n",
    "#texts = [\n",
    "#    \"A photo of a blue cylinder\",\n",
    "#    \"A photo of a cyan cylinder\",\n",
    "#    \"A photo of a blue cube\",\n",
    "#    \"A photo of a brown sphere\",\n",
    "#    \"A photo of a yellow cube\"\n",
    "#]\n",
    "#target_label = \"A photo of a blue cylinder\"\n",
    "#\n",
    "#\n",
    "###### IMAGE 4 #####\n",
    "### RELATIONAL ##\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/sphere_left_cone/CLEVR_rel_000007.png\"\n",
    "#texts = [\n",
    "#    \"A photo of a sphere left of a cone\",\n",
    "#    \"A photo of a sphere right of a cone\",\n",
    "#    \"A photo of a cone left of a sphere\",\n",
    "#    \"A photo of a cube left of a cone\",\n",
    "#    \"A photo of a cylinder right of a cone\"\n",
    "#]\n",
    "#target_label = \"A photo of a sphere left of a cone\"\n",
    "#\n",
    "### TWO OBJECT ##\n",
    "#texts = [\n",
    "#    \"A photo of a purple cone\",\n",
    "#    \"A photo of a blue cone\",\n",
    "#    \"A photo of a purple sphere\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a yellow cube\"\n",
    "#]\n",
    "#target_label = \"A photo of a purple cone\"\n",
    "\n",
    "\n",
    "########################################\n",
    "############# FINAL IMAGES #############\n",
    "########################################\n",
    "\n",
    "################################################################################################\n",
    "## Image 1 in final_gradcam\n",
    "## Relational\n",
    "#texts = [\n",
    "#    \"A photo of a cylinder left of a cone\",\n",
    "#    \"A photo of a cylinder right of a cone\",\n",
    "#    \"A photo of a cone left of a cylinder\",\n",
    "#    \"A photo of a cube right a cylinder\",\n",
    "#    \"A photo of a sphere right of a cone\",]\n",
    "#target_label = \"A photo of a cylinder left of a cone\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "#\n",
    "## Two object\n",
    "#texts = [\n",
    "#    \"A photo of a purple cylinder\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a purple cone\",\n",
    "#    \"A photo of a red sphere\",\n",
    "#    \"A photo of a blue cube\"\n",
    "#]\n",
    "#target_label = \"A photo of a purple cylinder\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_cone/CLEVR_rel_000020.png\"\n",
    "\n",
    "################################################################################################\n",
    "# Image 2 in final_gradcam\n",
    "# Relational, correct label\n",
    "texts = [\n",
    "    \"A photo of a cylinder left of a sphere\",\n",
    "    \"A photo of a cylinder right of a sphere\",\n",
    "    \"A photo of a sphere left of a cylinder\",\n",
    "    \"A photo of a cube right a cone\",\n",
    "    \"A photo of a sphere right of a cone\",]\n",
    "target_label = \"A photo of a cylinder left of a sphere\"\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "\n",
    "\n",
    "# Relational, predicted label\n",
    "texts = [\n",
    "    \"A photo of a cylinder left of a sphere\",\n",
    "    \"A photo of a cylinder right of a sphere\",\n",
    "    \"A photo of a sphere left of a cylinder\",\n",
    "    \"A photo of a cube right a cone\",\n",
    "    \"A photo of a sphere right of a cone\",]\n",
    "target_label = \"A photo of a cylinder right of a sphere\"\n",
    "image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Two object\n",
    "#texts = [\n",
    "#    \"A photo of a green sphere\",\n",
    "#    \"A photo of a blue sphere\",\n",
    "#    \"A photo of a green cylinder\",\n",
    "#    \"A photo of a red cone\",\n",
    "#    \"A photo of a purple cube\",\n",
    "#]\n",
    "#target_label = \"A photo of a green sphere\"\n",
    "#image_path = \"/home/bboulbarss/large_dataset/relational/train/cylinder_left_sphere/CLEVR_rel_000031.png\"\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "compute_gradcam_clip(image_path, texts, target_label, model, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044026d0-57cc-4b26-a851-617e3602a3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
